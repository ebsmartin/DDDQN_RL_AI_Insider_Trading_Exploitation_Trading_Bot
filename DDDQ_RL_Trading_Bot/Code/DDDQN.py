import os

# torch imports
import torch

class DDDQN(torch.nn.Module):
    def __init__(self, input_features, window_size):
        super().__init__()
        self.input_size = input_features * window_size  # input size
        print(f'Inside DDDQN: Input features - {input_features}')
        print(f'Inside DDDQN: Input size - {self.input_size}')
        print(f'Inside DDDQN: Window size - {window_size}')
        # The use of LeakyReLU activation functions can help alleviate the vanishing gradient problem
        self.leaky_relu = torch.nn.LeakyReLU(negative_slope=0.1) 
        
        # Dueling DQN architecture
        self.d1 = torch.nn.Linear(self.input_size, 256)  # input layer
        self.bn1 = torch.nn.BatchNorm1d(256)  # batch normalization  used to normalize the input layer by adjusting and scaling the activations
        self.d2 = torch.nn.Linear(256, 512)  # hidden layer
        self.bn2 = torch.nn.BatchNorm1d(512)  # batch normalization used to normalize the input layer by adjusting and scaling the activations
        self.drop1 = torch.nn.Dropout(0.5)  # dropout layer used to prevent overfitting
        self.d3 = torch.nn.Linear(512, 512)  # hidden layer
        self.bn3 = torch.nn.BatchNorm1d(512)  # batch normalization used to normalize the input layer by adjusting and scaling the activations
        self.drop2 = torch.nn.Dropout(0.5)  # dropout layer used to prevent overfitting
        self.d4 = torch.nn.Linear(512, 256)  # hidden layer
        self.bn4 = torch.nn.BatchNorm1d(256)    # batch normalization used to normalize the input layer by adjusting and scaling the activations
        self.drop3 = torch.nn.Dropout(0.3)  # dropout layer used to prevent overfitting
        self.dv1 = torch.nn.Linear(256, 128)  # value hidden layer
        self.da1 = torch.nn.Linear(256, 128)  # actions hidden layer
        self.dv2 = torch.nn.Linear(128, 1)  # value output
        self.da2 = torch.nn.Linear(128, 4)  # actions output

    def forward(self, input_data):
        # Debugging input shape
        print(f'Initial input data shape: {input_data.shape}')

        # Reshape input data to [batch_size, -1]
        if input_data.dim() > 2:  # More than two dimensions indicates it hasn't been flattened
            input_data = input_data.reshape(input_data.size(0), -1)
        
        # Debugging flattened shape
        print(f'Flattened input size: {input_data.size()}')
        
        # Ensure the input data is the correct shape for the first linear layer
        if input_data.size(1) != self.input_size:
            raise ValueError(f"Expected input size {self.input_size}, but got {input_data.size(1)}")

        # Neural network operations
        x = self.leaky_relu(self.d1(input_data))
        x = self.bn1(x)
        x = self.leaky_relu(self.d2(x))
        x = self.bn2(x)
        x = self.drop1(x)
        x = self.leaky_relu(self.d3(x))
        x = self.bn3(x)
        x = self.drop2(x)
        x = self.leaky_relu(self.d4(x))
        x = self.bn4(x)
        x = self.drop3(x)
        v = self.leaky_relu(self.dv1(x))
        a = self.leaky_relu(self.da1(x))
        v = self.dv2(v)
        a = self.da2(a)
        Q = v + (a - torch.mean(a, dim=1, keepdim=True))
        return Q

    
    # advantage function is used to calculate the advantage of taking each action at each state
    # processes the input state through the common layers and then the advantage stream layers.
    # used for debugging or analysis purposes.
    def advantage(self, state):
        # Common processing
        x = self.leaky_relu(self.d1(state))
        x = self.bn1(x)
        x = self.leaky_relu(self.d2(x))
        x = self.bn2(x)
        x = self.drop1(x)
        x = self.leaky_relu(self.d3(x))
        x = self.bn3(x)
        x = self.drop2(x)
        x = self.leaky_relu(self.d4(x))
        x = self.bn4(x)
        x = self.drop3(x)

        # Advantage stream processing
        a = self.leaky_relu(self.da1(x))  # actions function 
        a = self.da2(a) # actions output

        return a  # return advantage values
