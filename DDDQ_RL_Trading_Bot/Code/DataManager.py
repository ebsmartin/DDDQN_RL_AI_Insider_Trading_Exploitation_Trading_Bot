import pandas as pd
import os
import datetime

from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from sklearn.model_selection import train_test_split

from FinancialDataset import FinancialDataset

class DataManager:
    def __init__(self, directory):
        # Dictionary to store the key:ACCESSION_NUMBER value: [normalized_df, reference df]
        self.data_dict = {}
        self.prepare_data(directory)
        self.train_dataset = FinancialDataset(self.train_data_dict)
        self.test_dataset = FinancialDataset(self.test_data_dict)

    def prepare_data(self, directory):
        all_data_df = pd.DataFrame()
        for filename in os.listdir(directory):
            if filename.endswith(".csv"):
                print(filename)
                file_path = os.path.join(directory, filename)
                df = pd.read_csv(file_path, low_memory=False)  
                all_data_df = pd.concat([all_data_df, df], ignore_index=True)

        print('Number of unique asseccions:', all_data_df['ACCESSION_NUMBER'].nunique())
        
        print(f'Number of rows with missing values: {all_data_df.isnull().sum().sum()}')
        # Drop rows with missing values
        all_data_df = all_data_df.dropna(axis=0, how='any')

        print('Number of unique asseccions after dropping nan:', all_data_df['ACCESSION_NUMBER'].nunique())

        # ensure dates are in datetime format
        # timestamp is 2022-01-04 10:00:00, FILING_DATE is 2022-09-01

        all_data_df['FILING_DATE'] = pd.to_datetime(all_data_df['FILING_DATE'], errors='coerce')
        all_data_df['timestamp'] = pd.to_datetime(all_data_df['timestamp'], errors='coerce')

        # Count the number of NaT values
        num_failed_filing_date = all_data_df['FILING_DATE'].isna().sum()
        num_failed_timestamp = all_data_df['timestamp'].isna().sum()

        print(f"Number of rows where 'FILING_DATE' failed to infer: {num_failed_filing_date}")
        print(f"Number of rows where 'timestamp' failed to infer: {num_failed_timestamp}")

        # Drop rows where datetime conversion failed (NaT values)
        all_data_df.dropna(subset=['FILING_DATE', 'timestamp'], inplace=True)

        # Convert dates to Unix timestamps
        unix_epoch = pd.Timestamp("1970-01-01")
        all_data_df['FILING_DATE'] = (all_data_df['FILING_DATE'] - unix_epoch) // pd.Timedelta('1s')
        all_data_df['timestamp'] = (all_data_df['timestamp'] - unix_epoch) // pd.Timedelta('1s')
        
        # ensure close is a float
        all_data_df['close'] = all_data_df['close'].astype(float)
        # ensure ACCESSION_NUMBER is a string
        all_data_df['ACCESSION_NUMBER'] = all_data_df['ACCESSION_NUMBER'].astype(str)
        # ensure ISSUERTRADINGSYMBOL is a string
        all_data_df['ISSUERTRADINGSYMBOL'] = all_data_df['ISSUERTRADINGSYMBOL'].astype(str)

        # Group the DataFrame by the 'ACCESSION_NUMBER' column
        groups = all_data_df.groupby('ACCESSION_NUMBER')
        # Iterate over the groups
        for accession_number, dataframe in groups:
            # 'name' is the unique value of 'ACCESSION_NUMBER' for this group
            # 'group' is a DataFrame containing only the rows with this 'ACCESSION_NUMBER'
            # Create non_normalized_data_df
            reference_data_group = self.create_reference_data(dataframe)
            # Normalize the group
            normalized_data_group = self.normalize_data(dataframe)
            # Add the group and the normalized group to the dictionary
            self.data_dict[accession_number] = [normalized_data_group, reference_data_group]
        print(f'Number items in data_dict: ', len(self.data_dict))
        self.split_data()

    def normalize_data(self, data):
        # Drop the non-numeric columns
        data = data.drop(columns=['ACCESSION_NUMBER', 'FILING_DATE', 'PERIOD_OF_REPORT', 'ISSUERTRADINGSYMBOL', 
                                'ND_T_TRANS_DATE', 'D_T_TRANS_DATE', 'timestamp', 'close'])
        # Normalize the data
        normalized_data_df = pd.DataFrame(MinMaxScaler().fit_transform(data), columns=data.columns)
        return normalized_data_df


    def create_reference_data(self, data):
        reference_data_df = data[['FILING_DATE', 'timestamp', 'close']]
        return reference_data_df
    
    def split_data(self, test_size=0.25):
        # Get the keys of the data_dict
        keys = list(self.data_dict.keys())
        # Split the keys into a training set and a test set
        train_keys, test_keys = train_test_split(keys, test_size=test_size)
        # Create the train_data_dict and test_data_dict
        self.train_data_dict = {key: self.data_dict[key] for key in train_keys}
        self.test_data_dict = {key: self.data_dict[key] for key in test_keys}
        print(f'Number of items in train_data_dict: {len(self.train_data_dict)}')
        print(f'Number of items in test_data_dict: {len(self.test_data_dict)}')

    def get_data_loader(self, is_train=True, batch_size=1, num_workers=1, is_distributed=False):
        dataset = self.train_dataset if is_train else self.test_dataset
        sampler = DistributedSampler(dataset, shuffle=True) if is_distributed else None
        # Pass the custom collate function to DataLoader
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True, collate_fn=custom_collate_fn)


def custom_collate_fn(batch):
    # Assuming each element in batch is a tuple (normalized_tensor, reference_tensor, accession_number)
    normalized_tensors, reference_tensors, accession_numbers = zip(*batch)

    # Since batch size is 1, just return the single elements in the batch directly
    # If batch size could be more than 1, handle accordingly, possibly with padding or other methods
    return normalized_tensors[0], reference_tensors[0], accession_numbers[0]
