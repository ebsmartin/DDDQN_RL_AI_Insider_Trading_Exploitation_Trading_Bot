import numpy as np
import torch
from SumTree import SumTree

class PrioritizedExpReplay:
    """
    A class for implementing Prioritized Experience Replay using a SumTree.
    This allows more frequently for experiences where the model has a lot to learn,
    improving training efficiency and effectiveness.
    
    Attributes:
        tree (SumTree): Data structure to store experiences and their priorities.
        alpha (float): Determines how much prioritization is used (0 means no prioritization).
        beta (float): Controls the amount of importance sampling (0 means no correction).
        device (torch.device): Device on which tensors will be allocated (e.g., CUDA GPU).
        buffer_size (int): Maximum number of experiences the buffer can hold.
        window_size (int): Number of previous observations used to form a state.
        num_features (int): Number of features in each state observation.
    """

    def __init__(self, num_features, window_size, device, buffer_size=1000000, alpha=0.6, beta=0.4):
        self.tree = SumTree(buffer_size)
        self.alpha = alpha
        self.beta = beta
        self.device = device
        self.buffer_size = buffer_size
        self.window_size = window_size
        self.num_features = num_features

    def add_exp(self, state, action, reward, next_state):
        """
        Adds an experience to the buffer with maximum priority to ensure it is sampled at least once.
        
        Args:
            state: The state from which the action was taken.
            action: The action taken in the state.
            reward: The reward received after taking the action.
            next_state: The next state reached after taking the action.
        """
        if not (isinstance(state, np.ndarray) and isinstance(next_state, np.ndarray)):
            raise ValueError("States and next_states must be numpy arrays.")
        if not (isinstance(action, int) and isinstance(reward, float)):
            raise ValueError("Action must be an integer and reward must be a float.")

        max_priority = np.max(self.tree.tree[-self.tree.capacity:]) if self.tree.n_entries > 0 else 1.0
        self.tree.add((state, action, reward, next_state), max_priority ** self.alpha)

    def sample_exp(self, batch_size=64):
        batch, idxs, priorities = [], [], []
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            s = np.random.uniform(segment * i, segment * (i + 1))
            idx, p, exp = self.tree.get(s)
            priorities.append(p)
            batch.append(exp)
            idxs.append(idx)

        sampling_probabilities = np.array(priorities) / self.tree.total()
        is_weights = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)
        is_weights /= np.max(is_weights)  # Normalize for stability

        states, actions, rewards, next_states = zip(*batch)
        max_length = max([s.shape[0] for s in states])  # Ensuring max_length calculation

        # Ensure all states and next_states are padded to the same length
        padded_states = [np.pad(s, ((0, max_length - s.shape[0]), (0, 0)), mode='constant') if s.shape[0] < max_length else s for s in states]
        padded_next_states = [np.pad(s, ((0, max_length - s.shape[0]), (0, 0)), mode='constant') if s.shape[0] < max_length else s for s in next_states]

        states = torch.tensor(np.array(padded_states), dtype=torch.float32, device=self.device)
        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device)
        next_states = torch.tensor(np.array(padded_next_states), dtype=torch.float32, device=self.device)
        is_weights = torch.tensor(is_weights, dtype=torch.float32, device=self.device)

        return states, actions, rewards, next_states, idxs, is_weights



    def update_priorities(self, idxs, td_errors):
        """
        Updates the priorities of sampled experiences based on the TD errors.
        
        Args:
            idxs (list): Indices of the sampled experiences.
            td_errors (numpy.array): TD errors corresponding to each sampled experience.
        """
        priorities = np.abs(td_errors) + 1e-5  # Avoid zero priorities
        for idx, priority in zip(idxs, priorities):
            self.tree.update(idx, priority ** self.alpha)
