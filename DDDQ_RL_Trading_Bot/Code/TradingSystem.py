# %%
import numpy as np
import pandas as pd
import json
import os
from time import time
import logging


# Torch imports
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group
from torch.distributed import init_process_group, destroy_process_group

# Import classes
from Agent import Agent
from DataVisualizer import DataVisualizer
from DataManager import DataManager
from NumpyEncoder import NumpyEncoder
from Util import SetupInfo

# set Rank environment variables
# os.environ['RANK'] = '0'
# os.environ['WORLD_SIZE'] = '1'

# def setup_distributed(local_rank):
#     # Set environment variables
#     os.environ['MASTER_ADDR'] = 'localhost'
#     os.environ['MASTER_PORT'] = '12355'
#     os.environ['WORLD_SIZE'] = '1'
#     os.environ['RANK'] = '0'

#     # Initialize the process group
#     if not dist.is_initialized():
#         dist.init_process_group(backend='gloo', init_method='env://')
#     torch.cuda.set_device(local_rank)

#     return local_rank

def setup_distributed():
    rank = int(os.environ.get('SLURM_PROCID', 0))
    world_size = int(os.environ.get('SLURM_NTASKS', 1))
    local_rank = int(os.environ.get('SLURM_LOCALID', 0))
    
    torch.cuda.set_device(local_rank)
    init_process_group(
        backend='nccl',
        world_size=world_size,
        rank=rank
    )

    print(f"Running on Rank {rank}, Local Rank {local_rank}, using GPU {torch.cuda.current_device()}")


def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

class TradingSystem:
    def __init__(self, directory, local_rank=0):
        self.directory = directory
        self.local_rank = local_rank
        self.NUM_EPISODES = None
        self.trading_agent = None
        self.data_manager = DataManager(directory)
        self.train_df_shape = None
        self.test_df_shape = None
        self.data_visualizer = None

    def manual_data_partition(self, data):
        total_size = len(data)
        per_gpu = total_size // dist.get_world_size()
        start = self.local_rank * per_gpu
        end = start + per_gpu if self.local_rank < dist.get_world_size() - 1 else total_size
        return data[start:end]

    def setup_trading_agent(self):
        self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES) 
        if os.path.exists("Output/online_model/model.pt"):
            print("Loading the model")
            self.trading_agent.load_model()
        else:
            print("No model found, training from scratch")

        if torch.cuda.is_available():
            self.trading_agent.model = self.trading_agent.model.cuda(self.local_rank)
        self.trading_agent.model = DDP(self.trading_agent.model, device_ids=[self.local_rank])
        
    # Curriculum 1 
    def train(self):
        self.NUM_EPISODES = len(self.data_manager.train_data_dict)
        episode_mem = [{"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": [], "Reward": [], "Epsilon": [], "MSE Loss": []} for _ in range(self.NUM_EPISODES)]
        t0 = time()
        episode_num = 0
        print(f'Episodes total : {self.NUM_EPISODES}')
        for accession_number, data in self.data_manager.train_data_dict.items():
            normalized_df, reference_df = data
            self.train_df_shape = normalized_df.shape
            print(f'train_df_shape {self.train_df_shape}')
            self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES, self.local_rank)
            print(f"\n===== Episode {episode_num + 1} : {accession_number} =====")
            self.trading_agent.inventory = []
            state = self.trading_agent.get_state(0, normalized_df)
            portfolio_value = 0
            balance = 0
            self.trading_agent.portfolio = [0, balance, portfolio_value]
            print(len(normalized_df))
            for t in range(len(normalized_df) - 1):
                action = self.trading_agent.get_action(state, t, reference_df)
                
                if action == 0:
                    episode_mem[episode_num]["Sales"].append(reference_df['close'].iloc[t])
                elif action == 2:
                    episode_mem[episode_num]["Purchases"].append(reference_df['close'].iloc[t])
                
                next_state = self.trading_agent.get_state(t + 1, normalized_df)
                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, len(normalized_df)-1, trading_fee_rate=0.01)

                self.trading_agent.memory.add_exp(state, action, reward, next_state)
                loss = self.trading_agent.train() or 0
                state = next_state

                assets_held, balance, portfolio_value = self.trading_agent.portfolio

                episode_mem[episode_num]["Actions"].append(int(action))
                episode_mem[episode_num]["Inventory Size"].append(len(self.trading_agent.inventory))
                episode_mem[episode_num]["Portfolio Value"].append(portfolio_value)
                episode_mem[episode_num]["Balance"].append(balance)
                episode_mem[episode_num]["Reward"].append(reward)
                episode_mem[episode_num]["Epsilon"].append(self.trading_agent.epsilon)
                episode_mem[episode_num]["MSE Loss"].append(loss)

                # last 10 rewards 
                
                last_3_rewards = episode_mem[episode_num]["Reward"][-3:]
                if t % 50 == 0:
                    print(f"""
                            Time step {t} / {len(normalized_df)}   
                            |   Inventory Size: {len(self.trading_agent.inventory)}
                            |   Last 3 Actions: {episode_mem[episode_num]["Actions"][-3:]}
                            |   Portfolio Value: {round(episode_mem[episode_num]['Portfolio Value'][t], 3)} 
                            |   Balance: {round(episode_mem[episode_num]['Balance'][t], 3)}
                            |   {reference_df['ISSUERTRADINGSYMBOL'].iloc[t]}: ${round(reference_df['close'].iloc[t], 3)}   
                            |   Epsilon: {round(self.trading_agent.epsilon, 4)}   
                            |   MSE Loss: {loss}
                            |   Last 3 Rewards: {last_3_rewards}
                            |   Average Reward: {np.mean(episode_mem[episode_num]["Reward"])}
                            |   Max Reward: {max(episode_mem[episode_num]["Reward"])}
                            |   Min Reward: {min(episode_mem[episode_num]["Reward"])}
                            """)
            episode_num += 1
            self.trading_agent.save_model()

        total_sell_actions = sum(episode['Actions'].count(0) for episode in episode_mem)
        total_buy_actions = sum(episode['Actions'].count(2) for episode in episode_mem)
        total_hold_actions = sum(episode['Actions'].count(1) for episode in episode_mem)
        average_reward = np.mean([sum(episode['Reward']) for episode in episode_mem])
        average_balance = np.mean([sum(episode['Balance']) for episode in episode_mem])

        with open('Output/training_scores.out', 'a') as f:
            f.write(f"""
                    EPISODES Completed {episode_num + 1} |  (runtime: {time() - t0})   
                    | Total Sell Actions taken: {total_sell_actions}   |
                    Total Buy Actions taken: {total_buy_actions}   |
                    Total Hold Actions taken: {total_hold_actions}   |
                    Average Portfolio Value: {np.mean([sum(episode['Portfolio Value']) for episode in episode_mem])}   |
                    Average Balance: {average_balance}   |
                    Average Reward: {average_reward}   |
                    Epsilon is {round(self.trading_agent.epsilon, 3)}   |   
                    MSE Loss is {round(episode_mem[episode_num - 1]['MSE Loss'][-1], 3)}\n
                    """)
        with open("Output/episode_mem.json", 'w') as f:
            json.dump(episode_mem, f, cls=NumpyEncoder)

    def test(self):
        testing_mem = {"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": [], "Reward": []}
        t0 = time()
        self.NUM_EPISODES = len(self.data_manager.test_data_dict)
        
        for accession_number, data in self.data_manager.test_data_dict.items():
            normalized_df, reference_df = data
            self.test_df_shape = normalized_df.shape
            self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES, self.local_rank)
            self.trading_agent.epsilon = 0
            self.trading_agent.inventory = []
            state = self.trading_agent.get_state(0, normalized_df)
            balance = 0
            portfolio_value = 0
            self.trading_agent.portfolio = [0, balance, portfolio_value]

            for t in range(len(normalized_df) - 1):
                action = self.trading_agent.get_action(state, t, reference_df)

                if action == 0:
                    testing_mem["Sales"].append(reference_df['close'].iloc[t])
                elif action == 2:
                    testing_mem["Purchases"].append(reference_df['close'].iloc[t])

                next_state = self.trading_agent.get_state(t + 1, normalized_df)
                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, len(normalized_df)-1, trading_fee_rate=0.01)
                
                state = next_state
                assets_held, balance, portfolio_value = self.trading_agent.portfolio

                testing_mem["Actions"].append(int(action))
                testing_mem["Inventory Size"].append(len(self.trading_agent.inventory))
                testing_mem["Portfolio Value"].append(float(portfolio_value))
                testing_mem["Balance"].append(float(balance))
                testing_mem["Reward"].append(float(reward))
                if t % 10 == 0:
                    print(f"""
                            Time step {t} / {len(normalized_df)}   
                            |  Inventory Size: {len(self.trading_agent.inventory)}  
                            |  {reference_df['ISSUERTRADINGSYMBOL'].iloc[t]}: ${round(reference_df['close'].iloc[t], 3)}
                            |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   
                            |  Balance: {round(testing_mem['Balance'][t], 3)}
                            |  Action: {int(action)}  |  Reward: {round(reward, 3)}
                            |  Reward: {round(reward, 3)}
                            """)
    
        if self.local_rank == 0:

            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""TESTING (runtime: {time() - t0})   |  
                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\n
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)
        
        elif torch.distributed.is_available() and torch.distributed.is_initialized() and dist.rank == 0:
            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""TESTING (runtime: {time() - t0})   |  
                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\n
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)

    def run(self, directory):
        print(f"PyTorch version: {torch.__version__}")
        print(f"GPU available: {torch.cuda.is_available()}, Count: {torch.cuda.device_count()}, Name: {torch.cuda.get_device_name(0)}")

        try:
            # Creating data loaders
            train_loader = self.data_manager.get_data_loader(is_train=True, batch_size=32, num_workers=4, is_distributed=True)
            test_loader = self.data_manager.get_data_loader(is_train=False, batch_size=32, num_workers=4, is_distributed=True)

            # Train and Test
            self.train(train_loader)
            self.test(test_loader)

        except Exception as e:
            logging.error("An error occurred", exc_info=True)
        finally:
            if dist.is_initialized():
                destroy_process_group()

        # Only save models or log information from one process to avoid conflicts
        if setup.is_main_process():
            self.trading_agent.save_model()
            print("Model saved")

        # self.data_visualizer = DataVisualizer()
        # self.data_visualizer.visualize_data()

if __name__ == "__main__":
    setup_logging()
    # Setup distributed environment
    setup = SetupInfo.SlurmSetup()  # Initialize Slurm setup
    print(f'Rank {setup.rank}: Starting up...')
    setup.establish_communication()
    print(f'Rank {setup.rank}: Communication is ready.')
    # Ensure you pass the local rank to the TradingSystem for proper GPU setup
    trading_system = TradingSystem(local_rank=setup.local_rank)

    directory = "/s/chopin/l/grad/ebmartin/cs535/Term_Project/Data_Input/"
    trading_system.run(directory)



