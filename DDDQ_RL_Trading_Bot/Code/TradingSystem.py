# %%
import numpy as np
import pandas as pd
import json
import os
from time import time
import logging


# Torch imports
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group
from torch.distributed import init_process_group, destroy_process_group

# Import classes
from Agent import Agent
from DataVisualizer import DataVisualizer
from DataManager import DataManager
from NumpyEncoder import NumpyEncoder
from Util import SetupInfo
# set Rank environment variables
# os.environ['RANK'] = '0'
# os.environ['WORLD_SIZE'] = '1'

def setup_distributed():
    local_rank = int(os.environ['SLURM_LOCALID'])
    world_size = int(os.environ['SLURM_NTASKS'])
    rank = int(os.environ['SLURM_PROCID'])

    os.environ['CUDA_VISIBLE_DEVICES'] = str(local_rank)
    torch.cuda.set_device(0)  # Access the only visible GPU.

    if not dist.is_initialized():
        dist.init_process_group(backend='gloo', world_size=world_size, rank=rank)
    print(f"Rank {rank}/{world_size}, Local Rank {local_rank}, using GPU {torch.cuda.current_device()}")

def initialize_distributed_backend():
    rank = int(os.environ.get('SLURM_PROCID', 0))
    world_size = int(os.environ.get('SLURM_NTASKS', 1))
    if not dist.is_initialized():
        dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)
    torch.cuda.set_device(int(os.environ['SLURM_LOCALID']))  # Set the device to local rank

def check_slurm_env_vars():
    slurm_vars = ['SLURM_LOCALID', 'SLURM_NTASKS', 'SLURM_PROCID', 'SLURM_JOB_NODELIST']
    for var in slurm_vars:
        value = os.getenv(var)
        if value is None:
            raise EnvironmentError(f"Environment variable {var} is not set. Please check your Slurm job configuration.")
        else:
            print(f"{var} = {value}")

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')



class TradingSystem:
    def __init__(self, directory, local_rank=0):
        self.directory = directory
        self.local_rank = local_rank
        self.NUM_EPISODES = None
        self.trading_agent = None
        self.data_manager = DataManager(directory)
        self.train_shape = None
        self.test_shape = None
        self.data_visualizer = None

    def manual_data_partition(self, data):
        total_size = len(data)
        per_gpu = total_size // dist.get_world_size()
        start = self.local_rank * per_gpu
        end = start + per_gpu if self.local_rank < dist.get_world_size() - 1 else total_size
        return data[start:end]

    def setup_trading_agent(self):
        self.trading_agent = Agent(self.train_shape, self.NUM_EPISODES) 
        if os.path.exists("Output/online_model/model.pt"):
            print("Loading the model")
            self.trading_agent.load_model()
        else:
            print("No model found, training from scratch")

        if torch.cuda.is_available():
            self.trading_agent.model = self.trading_agent.model.cuda(self.local_rank)
        self.trading_agent.model = DDP(self.trading_agent.model, device_ids=[self.local_rank])
        
    # Curriculum 1 
    def train(self, train_loader):
        self.NUM_EPISODES = len(train_loader)
        episode_mem = [{"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": [], "Reward": [], "Epsilon": [], "MSE Loss": []} for _ in range(self.NUM_EPISODES)]
        t0 = time()
        episode_num = 0
        print(f'Episodes total : {self.NUM_EPISODES}')
        for i, (normalized_tensor, reference_tensor, accession_number) in enumerate(train_loader):
            # Remove the batch dimension by using .squeeze()
            normalized_tensor = normalized_tensor.squeeze(0)
            self.train_shape = normalized_tensor.shape  # This should now be (time_steps, features)
            print(f'train_shape {self.train_shape}')
            self.trading_agent = Agent(self.train_shape, self.local_rank)
            print(f"\n===== Episode {episode_num + 1} : {accession_number} =====")
            self.trading_agent.inventory = []
            state = self.trading_agent.get_state(0, normalized_tensor)
            portfolio_value = 0
            balance = 0
            self.trading_agent.portfolio = [0, balance, portfolio_value]
            print(len(normalized_tensor))
            for t in range(normalized_tensor.size(0) - 1):
                action = self.trading_agent.get_action(state, t, reference_tensor)

                if action == 0:
                    episode_mem[episode_num]["Sales"].append(reference_tensor[t, 2].item())
                elif action == 2:
                    episode_mem[episode_num]["Purchases"].append(reference_tensor[t, 2].item())
                
                next_state = self.trading_agent.get_state(t + 1, normalized_tensor)
                reward = self.trading_agent.trade(t, action, normalized_tensor, reference_tensor, len(normalized_tensor)-1, trading_fee_rate=0.01)

                self.trading_agent.memory.add_exp(state, action, reward, next_state)
                loss = self.trading_agent.train() or 0
                state = next_state

                assets_held, balance, portfolio_value = self.trading_agent.portfolio

                episode_mem[episode_num]["Actions"].append(int(action))
                episode_mem[episode_num]["Inventory Size"].append(len(self.trading_agent.inventory))
                episode_mem[episode_num]["Portfolio Value"].append(portfolio_value)
                episode_mem[episode_num]["Balance"].append(balance)
                episode_mem[episode_num]["Reward"].append(reward)
                episode_mem[episode_num]["Epsilon"].append(self.trading_agent.epsilon)
                episode_mem[episode_num]["MSE Loss"].append(loss)

                # last 10 rewards 
                
                last_3_rewards = episode_mem[episode_num]["Reward"][-3:]
                if t % 50 == 0:
                    print(f"""
                            Time step {t} / {len(normalized_tensor)}   
                            |   Inventory Size: {len(self.trading_agent.inventory)}
                            |   Last 3 Actions: {episode_mem[episode_num]["Actions"][-3:]}
                            |   Portfolio Value: {round(episode_mem[episode_num]['Portfolio Value'][t], 3)} 
                            |   Balance: {round(episode_mem[episode_num]['Balance'][t], 3)}
                            |   Epsilon: {round(self.trading_agent.epsilon, 4)}   
                            |   MSE Loss: {loss}
                            |   Last 3 Rewards: {last_3_rewards}
                            |   Average Reward: {np.mean(episode_mem[episode_num]["Reward"])}
                            |   Max Reward: {max(episode_mem[episode_num]["Reward"])}
                            |   Min Reward: {min(episode_mem[episode_num]["Reward"])}
                            """)
            episode_num += 1
            self.trading_agent.save_model()

        total_sell_actions = sum(episode['Actions'].count(0) for episode in episode_mem)
        total_buy_actions = sum(episode['Actions'].count(2) for episode in episode_mem)
        total_hold_actions = sum(episode['Actions'].count(1) for episode in episode_mem)
        average_reward = np.mean([sum(episode['Reward']) for episode in episode_mem])
        average_balance = np.mean([sum(episode['Balance']) for episode in episode_mem])

        with open('Output/training_scores.out', 'a') as f:
            f.write(f"""
                    EPISODES Completed {episode_num + 1} |  (runtime: {time() - t0})   
                    | Total Sell Actions taken: {total_sell_actions}   |
                    Total Buy Actions taken: {total_buy_actions}   |
                    Total Hold Actions taken: {total_hold_actions}   |
                    Average Portfolio Value: {np.mean([sum(episode['Portfolio Value']) for episode in episode_mem])}   |
                    Average Balance: {average_balance}   |
                    Average Reward: {average_reward}   |
                    Epsilon is {round(self.trading_agent.epsilon, 3)}   |   
                    MSE Loss is {round(episode_mem[episode_num - 1]['MSE Loss'][-1], 3)}\n
                    """)
        with open("Output/episode_mem.json", 'w') as f:
            json.dump(episode_mem, f, cls=NumpyEncoder)

    def test(self, test_loader):
        testing_mem = {"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": [], "Reward": []}
        t0 = time()
        self.NUM_EPISODES = len(test_loader)
        
        for i, (normalized_tensor, reference_tensor, accession_number) in enumerate(test_loader):
            # Remove the batch dimension by using .squeeze()
            normalized_tensor = normalized_tensor.squeeze(0)
            self.test_shape = normalized_tensor.shape  # This should now be (time_steps, features)
            print(f'test_shape {self.test_shape}')
            self.trading_agent = Agent(self.test_shape, self.local_rank)
            self.trading_agent.epsilon = 0  # No exploration during testing
            self.trading_agent.inventory = []
            state = self.trading_agent.get_state(0, normalized_tensor)
            balance = 0
            portfolio_value = 0
            self.trading_agent.portfolio = [0, balance, portfolio_value]

            for t in range(len(normalized_tensor) - 1):
                action = self.trading_agent.get_action(state, t, reference_tensor)

                if action == 0:
                    testing_mem["Sales"].append(reference_tensor[t, 2].item())
                elif action == 2:
                    testing_mem["Purchases"].append(reference_tensor[t, 2].item())

                next_state = self.trading_agent.get_state(t + 1, normalized_tensor)
                reward = self.trading_agent.trade(t, action, normalized_tensor, reference_tensor, len(normalized_tensor)-1, trading_fee_rate=0.01)
                
                state = next_state
                assets_held, balance, portfolio_value = self.trading_agent.portfolio

                testing_mem["Actions"].append(int(action))
                testing_mem["Inventory Size"].append(len(self.trading_agent.inventory))
                testing_mem["Portfolio Value"].append(float(portfolio_value))
                testing_mem["Balance"].append(float(balance))
                testing_mem["Reward"].append(float(reward))
                if t % 10 == 0:
                    print(f"""
                            Time step {t} / {len(normalized_tensor)}   
                            |  Inventory Size: {len(self.trading_agent.inventory)}  
                            |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   
                            |  Balance: {round(testing_mem['Balance'][t], 3)}
                            |  Action: {int(action)}  |  Reward: {round(reward, 3)}
                            |  Reward: {round(reward, 3)}
                            """)
    
        if self.local_rank == 0:

            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""TESTING (runtime: {time() - t0})   |  
                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\n
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)
        
        elif torch.distributed.is_available() and torch.distributed.is_initialized() and dist.rank == 0:
            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""TESTING (runtime: {time() - t0})   |  
                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\n
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)

    def run(self):
        print(f"PyTorch version: {torch.__version__}")
        print(f"GPU available: {torch.cuda.is_available()}, Count: {torch.cuda.device_count()}, Name: {torch.cuda.get_device_name(0)}")

        try:

            # Creating data loaders
            train_loader = self.data_manager.get_data_loader(is_train=True, batch_size=32, num_workers=1, is_distributed=True)
            test_loader = self.data_manager.get_data_loader(is_train=False, batch_size=32, num_workers=1, is_distributed=True)

            # Train and Test
            self.train(train_loader)
            self.test(test_loader)

        except Exception as e:
            logging.error("An error occurred", exc_info=True)
        finally:
            if dist.is_initialized():
                dist.destroy_process_group()

        # Only save models or log information from one process to avoid conflicts
        if dist.get_rank() == 0:
            self.model.save_model()
            print("Model saved")

if __name__ == "__main__":
    print("Checking SLURM environment variables...")
    check_slurm_env_vars()

    # Initialize logging and Slurm setup
    setup_logging()
    setup = SetupInfo.SlurmSetup()  # This will use SLURM environment variables internally
    print(f'Rank {setup.local_rank}: Starting up...')
    setup.establish_communication()

    # Other setup and execution logic
    print("Testing GPU accessibility...")
    print("Available GPUs:", torch.cuda.device_count())
    print("Current CUDA Device ID:", torch.cuda.current_device())
    print("World Size:", setup.world_size)
    print("Rank:", setup.rank)
    print("Local Rank:", setup.local_rank)
    print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES"))

    os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

    initialize_distributed_backend()

    directory = "/s/chopin/l/grad/ebmartin/cs535/Term_Project/Data_Input/"
    trading_system = TradingSystem(directory, local_rank=setup.local_rank)
    trading_system.run()
