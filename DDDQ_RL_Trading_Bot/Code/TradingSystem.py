# %%
import numpy as np
import pandas as pd
import json
import os
from time import time
import logging


# Torch imports
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group
from torch.distributed import init_process_group, destroy_process_group

# Import classes
from Agent import Agent
from DataVisualizer import DataVisualizer
from DataManager import DataManager
from NumpyEncoder import NumpyEncoder
from Util import SetupInfo
# set Rank environment variables
# os.environ['RANK'] = '0'
# os.environ['WORLD_SIZE'] = '1'

def setup_distributed():
    local_rank = int(os.environ['SLURM_LOCALID'])
    world_size = int(os.environ['SLURM_NTASKS'])
    rank = int(os.environ['SLURM_PROCID'])

    # This maps local_rank to a specific MIG device
    # Assuming MIG devices are listed as 0 and 1 for simplicity. Adjust as necessary.
    os.environ['CUDA_VISIBLE_DEVICES'] = str(local_rank)  
    torch.cuda.set_device(0)  # the visible device should always be 0 after setting CUDA_VISIBLE_DEVICES

    if not torch.distributed.is_initialized():
        torch.distributed.init_process_group(backend='gloo', world_size=world_size, rank=rank)
    print(f"Rank {rank}/{world_size}, Local Rank {local_rank}, using GPU {torch.cuda.current_device()}")

def check_slurm_env_vars():
    slurm_vars = ['SLURM_LOCALID', 'SLURM_NTASKS', 'SLURM_PROCID', 'SLURM_JOB_NODELIST']
    for var in slurm_vars:
        value = os.getenv(var)
        if value is None:
            raise EnvironmentError(f"Environment variable {var} is not set. Please check your Slurm job configuration.")
        else:
            print(f"{var} = {value}")

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')



class TradingSystem:
    def __init__(self, directory, local_rank=0):
        self.directory = directory
        self.local_rank = local_rank
        self.NUM_EPISODES = 0
        self.trading_agent = None
        self.data_manager = DataManager(directory)
        self.train_shape = None
        self.test_shape = None
        self.data_visualizer = None
        
    # Curriculum 1 
    def train(self, train_loader, num_epochs=10):
        self.NUM_EPISODES = len(train_loader)
        # get example normalized_tensor, reference_tensor, accession_number from train_loader
        normalized_tensor_example, reference_tensor_example, accession_number_example = next(iter(train_loader))
        normalized_tensor_example = normalized_tensor_example.squeeze(0) # Remove the batch dimension by using .squeeze()
        self.train_shape = normalized_tensor_example.shape  # This should now be (time_steps, features)
        print(f'Normalized Tensor shape: {normalized_tensor_example.shape}')    
        # Initialize the trading agent 
        self.trading_agent = Agent(self.train_shape, self.local_rank)
        
        for epoch in range(num_epochs):
            print(f"\n===== Epoch {epoch + 1} =====")
            episode_mem = [{"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": [], "Reward": [], "Epsilon": [], "MSE Loss": []} for _ in range(self.NUM_EPISODES)]
            t0 = time()
            episode_num = 0
            # self.trading_agent.update_epsilon_epoch(epoch)

            print(f'Episodes total : {self.NUM_EPISODES}')
            for i, (normalized_tensor, reference_tensor, accession_number) in enumerate(train_loader):
                if normalized_tensor.ndim < 2:
                    # Log or handle cases where tensor does not have expected dimensions
                    print(f"Skipping dataset {accession_number} due to incorrect shape: {normalized_tensor.shape}")
                    continue
            
                # Remove the batch dimension by using .squeeze()
                normalized_tensor = normalized_tensor.squeeze(0)
        
                if len(self.train_shape) != 2 or self.train_shape[0] < 1:  # Expecting [batch_size, feature_size]
                    print(f"Error in dataset {accession_number}: Expected shape [batch_size, feature_size], got {self.train_shape}")
                    continue  # Skip this batch
                
                # reset the agent
                self.trading_agent.reset_agent(False)

                print(f"\n===== Episode {episode_num + 1} : {accession_number} =====")
                state = self.trading_agent.get_state(0, normalized_tensor)
                # print(f'state shape {state.shape}')
                state_np = state.cpu().detach().numpy() if isinstance(state, torch.Tensor) else state
                # print(f'state_np shape {state_np.shape}')
                portfolio_value = 0
                balance = 0
                print(len(normalized_tensor))
                for t in range(normalized_tensor.size(0) - 1):
                    action = self.trading_agent.get_action(state, t, reference_tensor, normalized_tensor.size(0))

                    if action == 0:
                        episode_mem[episode_num]["Sales"].extend(self.trading_agent.inventory)
                    elif action == 2:
                        episode_mem[episode_num]["Purchases"].append(reference_tensor[t, 2].item())
                    
                    next_state = self.trading_agent.get_state(t + 1, normalized_tensor)
                    next_state_np = next_state.cpu().detach().numpy() if isinstance(next_state, torch.Tensor) else next_state

                    reward = self.trading_agent.trade(t, action, normalized_tensor, reference_tensor, len(normalized_tensor)-1, trading_fee_rate=0.01)

                    self.trading_agent.memory.add_exp(state_np, int(action), float(reward), next_state_np)
                    loss = self.trading_agent.train() or 0
                    state = next_state

                    total_assets_purchased, balance, portfolio_value = self.trading_agent.portfolio

                    episode_mem[episode_num]["Actions"].append(int(action))
                    episode_mem[episode_num]["Inventory Size"].append(len(self.trading_agent.inventory))
                    episode_mem[episode_num]["Portfolio Value"].append(portfolio_value)
                    episode_mem[episode_num]["Balance"].append(balance)
                    episode_mem[episode_num]["Reward"].append(reward)
                    episode_mem[episode_num]["Epsilon"].append(self.trading_agent.epsilon)
                    episode_mem[episode_num]["MSE Loss"].append(loss)

                    # percentage gain based on total assets purchased and balance
                    percentage_proft = (balance / total_assets_purchased) * 100 if total_assets_purchased > 0 else 0
                    last_3_rewards = episode_mem[episode_num]["Reward"][-3:]
                    # if the last iteration of the episode
                    if t == len(normalized_tensor) - 2:
                        print(f"""
                                Time step {t + 1} / {len(normalized_tensor)}   
                                |   Inventory Size: {len(self.trading_agent.inventory)}
                                |   Last 3 Actions: {episode_mem[episode_num]["Actions"][-3:]}
                                |   Portfolio Value: {round(episode_mem[episode_num]['Portfolio Value'][t], 3)} 
                                |   Balance: {round(episode_mem[episode_num]['Balance'][t], 3)}
                                |   Percentage Profit: {round(percentage_proft, 3)}
                                |   Total Sales: {sum(episode_mem[episode_num]["Sales"])}
                                |   Total Purchases: {sum(episode_mem[episode_num]["Purchases"])}
                                |   Epsilon: {round(self.trading_agent.epsilon, 4)}   
                                |   MSE Loss: {loss}
                                |   Last 3 Rewards: {last_3_rewards}
                                |   Average Reward: {np.mean(episode_mem[episode_num]["Reward"])}
                                |   Max Reward: {max(episode_mem[episode_num]["Reward"])}
                                |   Min Reward: {min(episode_mem[episode_num]["Reward"])}
                                """)
                episode_num += 1
                if episode_num % 100 == 0:
                    print(f"Saving model at episode {episode_num}")
                    self.trading_agent.save_model()

            total_sell_actions = sum(episode['Actions'].count(0) for episode in episode_mem)
            total_buy_actions = sum(episode['Actions'].count(2) for episode in episode_mem)
            total_hold_actions = sum(episode['Actions'].count(1) for episode in episode_mem)
            average_reward = np.mean([reward for episode in episode_mem for reward in episode['Reward']])
            average_balance = np.mean([balance for episode in episode_mem for balance in episode['Balance']])

            with open(f'Output/training_scores_epoch_{epoch + 1}.out', 'a') as f:
                f.write(f"""
                        EPISODES Completed {episode_num + 1} |  (runtime: {time() - t0})   
                        | Total Sell Actions taken: {total_sell_actions}   |
                        Total Buy Actions taken: {total_buy_actions}   |
                        Total Hold Actions taken: {total_hold_actions}   |
                        Average Portfolio Value: {np.mean([sum(episode['Portfolio Value']) for episode in episode_mem])}   |
                        Average Balance: {average_balance}   |
                        Average Reward: {average_reward}   |
                        Total Balance: {sum([sum(episode['Balance']) for episode in episode_mem])}   |
                        Total Portfolio Value: {sum([sum(episode['Portfolio Value']) for episode in episode_mem])}   |
                        Epsilon is {round(self.trading_agent.epsilon, 3)}   |   
                        MSE Loss is {round(episode_mem[episode_num - 1]['MSE Loss'][-1], 3)}\n
                        """)
            with open(f"Output/episode_mem_epoch_{epoch + 1}.json", 'w') as f:
                json.dump(episode_mem, f, cls=NumpyEncoder)

    def test(self, test_loader):
        self.NUM_EPISODES = len(test_loader)
        testing_mem = [{"Actions": [], "Purchases": [], "Sales": [], "Inventory Size": [], "Portfolio Value": [], "Balance": []} for _ in range(self.NUM_EPISODES)]
        t0 = time()
        
        normalized_tensor_example, reference_tensor_example, accession_number_example = next(iter(test_loader))
        normalized_tensor_example = normalized_tensor_example.squeeze(0)  # Remove the batch dimension by using .squeeze()
        self.test_shape = normalized_tensor_example.shape  # This should now be (time_steps, features)
        print(f'Normalized Tensor shape:     {normalized_tensor_example.shape}')

        self.trading_agent = Agent(self.test_shape, self.local_rank)

        test_episode = 0
        print(f'Episodes total : {self.NUM_EPISODES}')
        for i, (normalized_tensor, reference_tensor, accession_number) in enumerate(test_loader):
            # Remove the batch dimension by using .squeeze()
            normalized_tensor = normalized_tensor.squeeze(0)
            self.trading_agent.reset_agent(True)
            self.trading_agent.epsilon = 0  # No exploration during testing
            state = self.trading_agent.get_state(0, normalized_tensor)
            balance = 0
            portfolio_value = 0
            print(f"\n===== Episode {test_episode + 1} : {accession_number} =====")
            print(f'Normalized Tensor shape: {normalized_tensor.shape}')
            for t in range(normalized_tensor.size(0) - 1):
                action = self.trading_agent.get_action(state, t, reference_tensor, normalized_tensor.size(0))

                if action == 0:
                    # This is not correct, it only is adding one value to the list but 
                    # the agent sells all inventory at once
                    testing_mem[test_episode]["Sales"].extend(self.trading_agent.inventory)
                elif action == 2:
                    testing_mem[test_episode]["Purchases"].append(reference_tensor[t, 2].item())

                next_state = self.trading_agent.get_state(t + 1, normalized_tensor)
                
                reward = self.trading_agent.trade(t, action, normalized_tensor, reference_tensor, len(normalized_tensor)-1, trading_fee_rate=0.01)

                state = next_state
                total_assets_purchased, balance, portfolio_value = self.trading_agent.portfolio

                testing_mem[test_episode]["Actions"].append(int(action))
                testing_mem[test_episode]["Inventory Size"].append(len(self.trading_agent.inventory))
                testing_mem[test_episode]["Portfolio Value"].append(portfolio_value)
                testing_mem[test_episode]["Balance"].append(balance)

                if t == len(normalized_tensor) - 2:
                    print(f"""
                            Time step {t + 1} / {len(normalized_tensor)}   
                            |  Inventory Size: {len(self.trading_agent.inventory)}  
                            |  Portfolio Value: {round(testing_mem[test_episode]['Portfolio Value'][t], 3)}   
                            |  Balance: {round(testing_mem[test_episode]['Balance'][t], 3)}
                            |  Percentage Profit: {round(float(balance / total_assets_purchased) * 100 if total_assets_purchased > 0 else 0, 3)}
                            """)
            test_episode += 1

        total_sell_actions = sum(episode['Actions'].count(0) for episode in testing_mem)
        total_buy_actions = sum(episode['Actions'].count(2) for episode in testing_mem)
        total_hold_actions = sum(episode['Actions'].count(1) for episode in testing_mem)
        average_balance = np.mean([balance for episode in testing_mem for balance in episode['Balance']])
        
        if self.local_rank == 0:
        
            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""
                        TESTING (runtime: {time() - t0})   |  
                        EPISODES Completed {test_episode + 1} |  (runtime: {time() - t0}) |
                        Total Sell Actions taken: {total_sell_actions}   |
                        Total Buy Actions taken: {total_buy_actions}   |
                        Total Hold Actions taken: {total_hold_actions}   |  
                        Total Purchases: {sum(sum(episode["Purchases"]) for episode in testing_mem)} |
                        Total Sales: {sum(sum(episode["Sales"]) for episode in testing_mem)} |
                        Average Balance: {average_balance}   |
                        Total Balance: {sum(sum(episode["Balance"]) for episode in testing_mem)}   |
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)
        
        elif torch.distributed.is_available() and torch.distributed.is_initialized() and dist.rank == 0:
            with open('Output/testing_scores.out', 'a') as f:
                f.write(f"""
                        TESTING (runtime: {time() - t0})   |  
                        EPISODES Completed {test_episode + 1} |  (runtime: {time() - t0}) |
                        Total Sell Actions taken: {total_sell_actions}   |
                        Total Buy Actions taken: {total_buy_actions}   |
                        Total Hold Actions taken: {total_hold_actions}   |  
                        Total Purchases: {sum([sum(episode['Purchases']) for episode in testing_mem])}   |
                        Total Sales: {sum(sum(episode["Sales"]) for episode in testing_mem)} |
                        Average Balance: {average_balance}   |
                        Total Balance: {sum(sum(episode["Balance"]) for episode in testing_mem)} |
                        """)
            with open("Output/testing_mem.json", 'w') as f:
                json.dump(testing_mem, f, cls=NumpyEncoder)

    def run(self):
        print(f"PyTorch version: {torch.__version__}")
        print(f"GPU available: {torch.cuda.is_available()}, Count: {torch.cuda.device_count()}, Name: {torch.cuda.get_device_name(0)}")

        try:

            # Creating data loaders
            train_loader = self.data_manager.get_data_loader(is_train=True, batch_size=1, num_workers=0, is_distributed=True)
            print(f"Number of training batches: {len(train_loader)}")  # This should approximately equal the ceiling of 3844 / 32

            test_loader = self.data_manager.get_data_loader(is_train=False, batch_size=1, num_workers=0, is_distributed=True)
            print(f"Number of testing batches: {len(test_loader)}")  # Similarly for testing data
            
            # Train and Test
            self.train(train_loader)
            self.test(test_loader)

        except Exception as e:
            logging.error("An error occurred", exc_info=True)
        finally:
            if dist.is_initialized():
                dist.destroy_process_group()

        # Only save models or log information from one process to avoid conflicts
        if self.local_rank == 0:
            print("Saving models and visualizing data...")
            self.trading_agent.save_model()
            # self.data_visualizer = DataVisualizer()
            # self.data_visualizer.visualize_data("Output/episode_mem.json", "Output/testing_mem.json")

if __name__ == "__main__":
    print("Checking SLURM environment variables...")
    check_slurm_env_vars()

    # Initialize logging and Slurm setup
    setup_logging()
    setup = SetupInfo.SlurmSetup()  # This will use SLURM environment variables internally
    print(f'Rank {setup.local_rank}: Starting up...')
    setup.establish_communication()

    

    # setup_distributed()
    print(f"Using CUDA device {torch.cuda.current_device()}")

    directory = "/s/chopin/l/grad/ebmartin/cs535/Term_Project/DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot/DDDQ_RL_Trading_Bot/Datasets/Cleaned_Engineered_Data/"
    trading_system = TradingSystem(directory, local_rank=setup.local_rank)
    trading_system.run()
