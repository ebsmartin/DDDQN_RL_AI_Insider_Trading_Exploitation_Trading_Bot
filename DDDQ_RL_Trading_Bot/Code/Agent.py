import numpy as np
import pandas as pd
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
from time import time
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import os
from collections import OrderedDict

# to run distributed training
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp

from DDDQN import DDDQN
from PrioritizedExpReplay import PrioritizedExpReplay

class Agent():

    # Initializes the agent with various parameters such as window_size, gamma, number of episodes, learning rate, minimum epsilon, and update interval. 
    # It also initializes the device, portfolio, epsilon, training step counter, memory, batch size, online and target networks, optimizer, scheduler, 
    # and the holding time.
    def __init__(self, data_shape, local_rank, window_size=39, gamma=0.90, update_interval=26, lr=0.001, min_epsilon=0.02):
        # Ensure data_shape is a tuple and has enough elements
        if isinstance(data_shape, torch.Size):
            self.data_shape = tuple(data_shape)  # Convert to tuple if coming directly as torch.Size
        if len(data_shape) < 2:
            raise ValueError("data_shape must have at least two dimensions. Current shape: {}".format(data_shape))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.window_size = window_size  # number of previous prices to use as state
        self.portfolio = [0.0, 0.0, 0.0]  # [total_assets, cash_held, total_portfolio_value (close_price * len(self.inventory) - sum(self.inventory))]
        self.inventory = [] 
        self.consecutive_buys = 0
        self.time_since_last_buy = 0
        self.gamma = gamma  # discount factor for future rewards
        self.epsilon = 1.0  # exploration rate
        self.min_epsilon = min_epsilon   # minimum exploration rate
        self.update_interval = update_interval  # number of steps between target network updates
        self.trainstep = 0
        self.memory = PrioritizedExpReplay(self.window_size, data_shape[1], self.device)
        self.batch_size = 64
        self.online_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net.load_state_dict(self.online_net.state_dict())
        initial_learning_rate = lr
        decay_rate = 0.9  # adjust this value to control the decay rate
        self.optimizer = optim.Adam(self.online_net.parameters(), lr=initial_learning_rate)
        self.scheduler = ExponentialLR(self.optimizer, gamma=decay_rate)
        self.criterion = nn.MSELoss()
        self.epsilon_decay = 0.9999  # exponential decay rate for epsilon
        self.local_rank = local_rank
        
        torch.cuda.set_device(self.local_rank)
        if not dist.is_initialized():
            dist.init_process_group(backend='gloo', init_method='env://')

        if os.path.exists("Output/online_model/model.pt"):
            print("Loading the model")
            self.load_model()
        else:
            print("No model found, training from scratch")

        self.online_net = self.online_net.to(self.device)
        self.target_net = self.target_net.to(self.device)
        self.online_net = DDP(self.online_net, device_ids=[self.local_rank], output_device=self.local_rank)
        self.target_net = DDP(self.target_net, device_ids=[self.local_rank], output_device=self.local_rank)

    # CURRICULUM TRAINING PART 1
    def get_action(self, state, t, reference_tensor, total_timesteps):
        # Ensure state_tensor is correctly shaped and on the right device
        state_tensor = state.to(self.device).unsqueeze(0)
        filing_date = reference_tensor[t, 0].item()
        timestamp = reference_tensor[t, 1].item()
        if timestamp < filing_date:  # if the current time is before the filing date, we can't trade
            return 3  # do nothing
        elif t == total_timesteps - 1:
            return 0 # sell all inventory
        elif torch.rand(1).item() <= self.epsilon:  # Using torch.rand for tensor operations
            if len(self.inventory) > 0:  # if we have inventory to sell
                return torch.randint(0, 3, (1,)).item()  # can sell, hold, or buy 
            else:
                return torch.randint(2, 4, (1,)).item()  # can only buy or do nothing
        else:
            with torch.no_grad():
                self.online_net.eval()
                actions = self.online_net(state_tensor)
                self.online_net.train()

                if len(self.inventory) > 0:
                        action = torch.argmax(actions[0, :3]).item()  # consider only sell, hold, or buy
                else:  # if we don't have inventory to sell, we remove that action option
                    action = torch.argmax(actions[0, 2:]).item() + 2
                return action

    def trade(self, t, action, asset_tensor, reference_tensor, total_timesteps, trading_fee_rate, profit_target=0.05):
        total_assets_purchased, balance, previous_portfolio_value = self.portfolio
        reward = 0
        time_decay_factor = 1 - (t / total_timesteps)  # decrease reward as time goes by
        realized_gains = 0  # Initialize realized gains
        unrealized_gains = 0  # Initialize unrealized gains

        timestamp = reference_tensor[t, 1].item()
        filing_date = reference_tensor[t, 0].item()
        close_price = reference_tensor[t, 2].item()

        # Force a sell action if it's the last timestep
        if t == total_timesteps - 1:
            if len(self.inventory) > 0:
                action = 0  # sell
            else:
                action = 3  # do nothing

        if timestamp > filing_date:
            if action == 0:  # sell
                sell_penalty = 0.01 * max(0, 10 - self.time_since_last_buy)  # decrease the reward if the agent sells too soon after a buy
                self.consecutive_buys = 0  # reset the count when a hold action is taken
                sold_total = len(self.inventory) * close_price
                profit_percentage = (sold_total - sum(self.inventory)) / sum(self.inventory)
                realized_gains = profit_percentage * time_decay_factor

                reward += realized_gains  # Add realized gains to the reward

                profit_factor = profit_percentage * time_decay_factor
                sharpe_ratio = self.calculate_sharpe_ratio(profit_percentage, sold_total, sum(self.inventory)) / 10
                sharpe_ratio = min(sharpe_ratio, 1)  # Ensure the ratio does not exceed 1
                adjusted_profit_factor = profit_factor * min(sharpe_ratio, 1)

                sell_penalty = 0.01 * max(0, 10 - self.time_since_last_buy)  # decrease the reward if the agent sells too soon after a buy
                adjusted_profit_factor -= sell_penalty  # subtract the sell penalty from the adjusted profit factor
                
                reward = adjusted_profit_factor - trading_fee_rate * (sold_total / sum(self.inventory))

                balance += (sold_total - sum(self.inventory))
                if profit_percentage >= profit_target:
                    reward += min(100 * sharpe_ratio, 1)  # Cap additional reward
                self.inventory = []
            elif action == 1:  # hold
                self.consecutive_buys = 0  # reset the count when a hold action is taken
                self.time_since_last_buy += 1 
                reward = -0.1 * time_decay_factor

            elif action == 2:  # buy
                self.inventory.append(close_price)
                total_assets_purchased += close_price
                buy_penalty = 0.01 * self.consecutive_buys  # increase the penalty as the number of consecutive buys increases
                reward = -trading_fee_rate * close_price * time_decay_factor - buy_penalty
                self.consecutive_buys += 1  # increment the count when a buy action is taken
                self.time_since_last_buy = 0  # reset the count when a buy action is taken
            elif action == 3:
                self.time_since_last_buy += 1 
                self.consecutive_buys = 0
                reward = 0.1  # do nothing

            current_portfolio_value = close_price * len(self.inventory) - sum(self.inventory)
            if len(self.inventory) > 0:
                unrealized_gains = (current_portfolio_value - previous_portfolio_value) / (previous_portfolio_value + 1e-10)
                reward += unrealized_gains * (0.5 if unrealized_gains < 0 else 2)  # Scale down losses more

            expected_reward = self.calculate_expected_reward(t, asset_tensor)
            td_error = reward - expected_reward
            adjusted_reward = reward + 0.5 * td_error
            

            self.portfolio = [total_assets_purchased, balance, current_portfolio_value]
            
            # Normalize the reward using logarithmic scaling to manage the scale of positive and negative rewards
            adjusted_reward = np.sign(adjusted_reward) * np.log(1 + abs(adjusted_reward))

            return adjusted_reward
        else:
            return 0  # No trade possible before the filing date


    def calculate_sharpe_ratio(self, profit, sold_price, bought_price):
        # Calculate return as a scalar since sold_price and bought_price are scalars
        returns = (sold_price - bought_price) / bought_price
        if not hasattr(self, 'return_list'):
            self.return_list = []
        self.return_list.append(returns)
        if len(self.return_list) > 400:  # Maintain only the last 1000 returns for performance
            self.return_list = self.return_list[-1000:]
        
        # Compute mean and standard deviation of returns
        mean_returns = np.mean(self.return_list)
        std_returns = np.std(self.return_list)
        
        if std_returns == 0:
            return 1  # Avoid division by zero; provide minimal Sharpe ratio
        
        sharpe_ratio = mean_returns / std_returns
        return max(1, sharpe_ratio)  # Ensure Sharpe ratio is at least 1


    def calculate_expected_reward(self, t, asset_tensor):
        # Extract the state from the asset tensor at time t
        state_tensor = self.get_state(t, asset_tensor)  # Ensure get_state returns a properly formatted tensor
        
        # Ensure the state tensor is on the right device and formatted for batch processing
        state_tensor = state_tensor.to(self.device).unsqueeze(0)  # Adds batch dimension if not already present
        
        # Assume the agent chooses the best known action according to the current policy
        with torch.no_grad():
            self.online_net.eval()  # Set the network to evaluation mode
            predicted_q_values = self.online_net(state_tensor)
            
            best_action = torch.argmax(predicted_q_values, dim=1)  # Correct axis for batch dimension
            expected_reward = predicted_q_values[0, best_action].item()  # Access the best action's Q-value
            
            self.online_net.train()  # Set the network back to training mode
        
        return expected_reward



    #  This method softly updates the target network by blending the weights of the online network and target network using the parameter tau.
    def update_target(self, tau=0.001):
        for target_param, online_param in zip(self.target_net.parameters(), self.online_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)

    #  Exponentially decays the epsilon value from its initial value to the minimum epsilon value over the course of the total training steps.
    def update_epsilon(self):
        if self.epsilon > self.min_epsilon:
            self.epsilon *= self.epsilon_decay  # Exponential decay

    # Trains the online network using the samples from the prioritized experience replay memory. 
    # It updates the target network periodically based on the update_interval. 
    # It also calculates the loss, applies the importance sampling weights, and updates the priorities in the buffer.
    def train(self):
        if self.memory.tree.n_entries < self.batch_size:
            return

        if self.trainstep % self.update_interval == 0:
            self.update_target()

        states, actions, rewards, next_states, idxs, is_weights = self.memory.sample_exp(self.batch_size)

        q_next_state_online_net = self.online_net(next_states)
        q_next_state_target_net = self.target_net(next_states)

        max_action = torch.argmax(q_next_state_online_net, dim=1).to(self.device)

        batch_index = torch.arange(self.batch_size, dtype=torch.int64).to(self.device)

        q_predicted = self.online_net(states)
        q_target = q_predicted.clone().detach()

        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action]

        td_errors = q_target - q_predicted
        td_errors = td_errors.detach().cpu().numpy()
        td_errors = td_errors[np.arange(self.batch_size), actions.cpu().numpy()]

        # Update priorities in the buffer
        self.memory.update_priorities(idxs, td_errors)

        loss = self.criterion(q_predicted, q_target).view(-1, 1)
        loss = (loss * is_weights).mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_epsilon()
        self.trainstep += 1
        return loss.item()

    def save_model(self):
        output_directory = "Output"
        online_model_directory = os.path.join(output_directory, "online_model")
        target_model_directory = os.path.join(output_directory, "target_model")

        os.makedirs(online_model_directory, exist_ok=True)
        os.makedirs(target_model_directory, exist_ok=True)

        for _ in range(3):  # try 3 times
            try:
                if isinstance(self.online_net, nn.DataParallel):
                    torch.save(self.online_net.module.state_dict(), os.path.join(online_model_directory, 'model.pt'))
                else:
                    torch.save(self.online_net.state_dict(), os.path.join(online_model_directory, 'model.pt'))

                if isinstance(self.target_net, nn.DataParallel):
                    torch.save(self.target_net.module.state_dict(), os.path.join(target_model_directory, 'model.pt'))
                else:
                    torch.save(self.target_net.state_dict(), os.path.join(target_model_directory, 'model.pt'))
                break  # if successful, break the loop
            except RuntimeError:
                time.sleep(1)  # wait for a second before retrying

    def load_model(self):
        for _ in range(3):  # try 3 times
            try:
                state_dict = torch.load('Output/online_model/model.pt')
                break  # if successful, break the loop
            except RuntimeError:
                time.sleep(1)  # wait for a second before retrying

        if torch.cuda.device_count() > 1:
            if not isinstance(self.online_net, nn.DataParallel):
                self.online_net = nn.DataParallel(self.online_net)
        else:
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k[7:]  # remove 'module.' prefix
                new_state_dict[name] = v
            state_dict = new_state_dict

        self.online_net.load_state_dict(state_dict)
    

    def get_state(self, idx, data_tensor):
        start_idx = max(0, idx - self.window_size + 1)
        end_idx = idx + 1
        state_tensor = data_tensor[start_idx:end_idx]

        # Log the shape to verify
        # print(f"Shape before padding: {state_tensor.shape}")

        # Ensure state_tensor is of float type (if not already)
        state_tensor = state_tensor.float()

        # Handling edge cases where there aren't enough previous states
        if state_tensor.size(0) < self.window_size:
            padding_size = (self.window_size - state_tensor.size(0), 0)  # Corrected padding size
            state_tensor = nn.functional.pad(state_tensor, (0, 0, padding_size[0], 0), mode='constant', value=0)
            # The pad arguments are (padding_left, padding_right, padding_top, padding_bottom)

        # Log shape after padding
        # print(f"Shape after padding: {state_tensor.shape}")

        return state_tensor

    def reset_agent(self, test):
        self.portfolio = [0.0, 0.0, 0.0]
        self.inventory = [] 
        self.trainstep = 0 
        self.consecutive_buys = 0
        if test:
            self.epsilon = 0.0  # No exploration during testing

    def update_epsilon(self, epoch):
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay ** epoch)