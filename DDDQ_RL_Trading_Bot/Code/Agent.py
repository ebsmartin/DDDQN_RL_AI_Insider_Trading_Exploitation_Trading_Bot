import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
from time import time
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import os
from collections import OrderedDict

# to run distributed training
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp

from DDDQN import DDDQN
from PrioritizedExpReplay import PrioritizedExpReplay

class Agent():

    # Initializes the agent with various parameters such as window_size, gamma, number of episodes, learning rate, minimum epsilon, and update interval. 
    # It also initializes the device, portfolio, epsilon, training step counter, memory, batch size, online and target networks, optimizer, scheduler, 
    # and the holding time.
    def __init__(self, data_shape, num_episodes, window_size=30, gamma=0.99, update_interval=30, lr=0.001, min_epsilon=0.02, local_rank=0):
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.window_size = window_size  # number of previous prices to use as state
        self.data_shape = data_shape  # [num_features, window_size]
        self.portfolio = [0, 0, 0]  # [total_assets, cash_held, total_portfolio_value (asset value + cash held - initial investment)]
        self.inventory = [] 
        self.gamma = gamma  # discount factor for future rewards
        self.num_episodes = num_episodes    # number of episodes
        self.epsilon = 1.0  # exploration rate
        self.min_epsilon = min_epsilon   # minimum exploration rate
        self.update_interval = update_interval  # number of steps between target network updates
        self.trainstep = 0
        self.memory = PrioritizedExpReplay(self.window_size, data_shape[1], self.device)
        self.batch_size = 64
        self.online_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.holding_time = 0 # Number of timesteps for which the agent has held the position
        initial_learning_rate = lr
        decay_steps = self.num_episodes * self.data_shape[0] // 10  # adjust the divisor to control the decay rate
        decay_rate = 0.9  # adjust this value to control the decay rate
        self.optimizer = optim.Adam(self.online_net.parameters(), lr=initial_learning_rate)
        self.scheduler = ExponentialLR(self.optimizer, gamma=decay_rate)
        self.criterion = nn.MSELoss()


        self.local_rank = local_rank
        torch.cuda.set_device(self.local_rank)
        if not dist.is_initialized():
            dist.init_process_group(backend='gloo', init_method='env://')

        if os.path.exists("Output/online_model/model.pt"):
            print("Loading the model")
            self.load_model()
        else:
            print("No model found, training from scratch")

        self.online_net = self.online_net.to(self.device)
        self.target_net = self.target_net.to(self.device)
        self.online_net = DDP(self.online_net, device_ids=[self.local_rank], output_device=self.local_rank)
        self.target_net = DDP(self.target_net, device_ids=[self.local_rank], output_device=self.local_rank)

    # CURRICULUM TRAINING PART 1
    def get_action(self, state, t, reference_df):
        state_numpy = np.array(state)
        state_tensor = torch.FloatTensor(state_numpy).to(self.device).unsqueeze(0)
        if reference_df['timestamp'].iloc[t] < reference_df['FILING_DATE'].iloc[t]:  # if the current time is before the filing date, we can't trade
            return 3  # do nothing
        elif np.random.rand() <= self.epsilon:
            if len(self.inventory) > 0:  # if we have inventory to sell
                return np.random.choice([0, 1, 2, 3]) # we can sell, hold, buy, or do nothing
            else:
                return np.random.choice([2, 3])  # we can only buy or do nothing
        else:
            with torch.no_grad():
                self.online_net.eval()
                actions = self.online_net(state_tensor)
                self.online_net.train()

                if len(self.inventory) > 0:
                    action = torch.argmax(actions[0, :]).item()
                else: # if we don't have inventory to sell, we remove that action option
                    action = torch.argmax(actions[0, 2:]).item() + 2
                return action

    def trade(self, t, action, asset_df, reference_df, INITIAL_INVESTMENT, total_timesteps, trading_fee_rate, profit_target=0.05):
        assets_held, balance, previous_portfolio_value = self.portfolio
        reward = 0
        time_decay_factor = 1 - (t / total_timesteps)  # decrease reward as time goes by

        if reference_df['timestamp'].iloc[t] > reference_df['FILING_DATE'].iloc[t]:
            if action == 0:  # sell
                sold_price = reference_df["close"].iloc[t]
                bought_price = self.inventory.pop()  # this sells the most recently bought asset
                profit = sold_price - bought_price
                sharpe_ratio = self.calculate_sharpe_ratio(profit, sold_price, bought_price)
                reward = (profit * time_decay_factor - trading_fee_rate * sold_price) * sharpe_ratio  # adjust reward by sharpe ratio
                if profit >= profit_target:  # if profit target is reached
                    reward += 100 * sharpe_ratio  # increase reward proportionally to the sharpe ratio
            elif action == 1:  # hold
                reward = -0.1 * time_decay_factor * assets_held  # holding penalty proportional to the value of the assets held
            elif action == 2 and balance >= reference_df["close"].iloc[t]:  # buy
                self.inventory.append(reference_df["close"].iloc[t])
                reward = -trading_fee_rate * reference_df["close"].iloc[t] * time_decay_factor  # deduct trading fee
            elif action == 3:
                reward = 0.1 # do nothing
        else:  # if the current time is before the filing date, we can't trade so don't punish or reward the agent
            reward = 0

        # Calculate the current portfolio value
        current_portfolio_value = balance + reference_df["close"].iloc[t] * len(self.inventory) - INITIAL_INVESTMENT
        # Add the change in portfolio value to the reward
        reward += (current_portfolio_value - previous_portfolio_value) * time_decay_factor

        # Temporal Difference Update
        expected_reward = self.calculate_expected_reward(t, asset_df)
        td_error = reward - expected_reward
        adjusted_reward = reward + 0.5 * td_error  # learning rate of 0.5

        return adjusted_reward

    def calculate_sharpe_ratio(self, profit, sold_price, bought_price):
        returns = (sold_price - bought_price) / bought_price
        if not hasattr(self, 'return_list'):
            self.return_list = []
        self.return_list.append(returns)
        if len(self.return_list) > 1000:  # Keep only the last 1000 returns
            self.return_list = self.return_list[-1000:]
        mean_returns = np.mean(self.return_list)
        std_returns = np.std(self.return_list)
        if std_returns == 0:
            return 1  # Avoid division by zero
        sharpe_ratio = mean_returns / std_returns
        return max(1, sharpe_ratio)

    def calculate_expected_reward(self, t, asset_df):
        # Extract the state from the asset data at time t
        state = self.get_state(t, asset_df)
        state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)
        
        # Assume the agent chooses the best known action according to the current policy
        with torch.no_grad():
            self.online_net.eval()  # Set the network to evaluation mode
            predicted_q_values = self.online_net(state_tensor)

            best_action = torch.argmax(predicted_q_values).item()
            expected_reward = predicted_q_values[0, best_action].item()
            self.online_net.train()  # Set the network back to training mode
        
        return expected_reward



    #  This method softly updates the target network by blending the weights of the online network and target network using the parameter tau.
    def update_target(self, tau=0.001):
        for target_param, online_param in zip(self.target_net.parameters(), self.online_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)

    #  Linearly decays the epsilon value from its initial value to the minimum epsilon value over the course of the total training steps.
    def update_epsilon(self):
        if self.epsilon > self.min_epsilon:
            # b = self.min_epsilon**(1/(self.num_episodes*self.data_shape[0]))
            # self.epsilon = b**self.trainstep # exponential decay
            self.epsilon -= (1.0 - self.min_epsilon) / (self.num_episodes * self.data_shape[0])   # linear decay

    # Trains the online network using the samples from the prioritized experience replay memory. 
    # It updates the target network periodically based on the update_interval. 
    # It also calculates the loss, applies the importance sampling weights, and updates the priorities in the buffer.
    def train(self):
        if self.memory.tree.n_entries < self.batch_size:
            return

        if self.trainstep % self.update_interval == 0:
            self.update_target()

        states, actions, rewards, next_states, dones, idxs, is_weights = self.memory.sample_exp(self.batch_size)

        q_next_state_online_net = self.online_net(next_states)
        q_next_state_target_net = self.target_net(next_states)

        max_action = torch.argmax(q_next_state_online_net, dim=1).to(self.device)

        batch_index = torch.arange(self.batch_size, dtype=torch.int64).to(self.device)

        q_predicted = self.online_net(states)
        q_target = q_predicted.clone().detach()

        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action] * dones

        td_errors = q_target - q_predicted
        td_errors = td_errors.detach().cpu().numpy()
        td_errors = td_errors[np.arange(self.batch_size), actions.cpu().numpy()]

        # Update priorities in the buffer
        self.memory.update_priorities(idxs, td_errors)

        loss = self.criterion(q_predicted, q_target).view(-1, 1)
        loss = (loss * is_weights).mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_epsilon()
        self.trainstep += 1
        return loss.item()

    def save_model(self):

        output_directory = "Output"
        online_model_directory = os.path.join(output_directory, "online_model")
        target_model_directory = os.path.join(output_directory, "target_model")

        os.makedirs(online_model_directory, exist_ok=True)
        os.makedirs(target_model_directory, exist_ok=True)

        # If the model is wrapped with DataParallel, save the state_dict of the module
        if isinstance(self.online_net, nn.DataParallel):
            torch.save(self.online_net.module.state_dict(), os.path.join(online_model_directory, 'model.pt'))
        else:
            torch.save(self.online_net.state_dict(), os.path.join(online_model_directory, 'model.pt'))

        if isinstance(self.target_net, nn.DataParallel):
            torch.save(self.target_net.module.state_dict(), os.path.join(target_model_directory, 'model.pt'))
        else:
            torch.save(self.target_net.state_dict(), os.path.join(target_model_directory, 'model.pt'))

    def load_model(self):
        # Load the state dictionary from the file
        state_dict = torch.load('Output/online_model/model.pt')

        # Check if the model is running on multiple GPUs or a single GPU
        if torch.cuda.device_count() > 1:
            # If the model is running on multiple GPUs, wrap the model with nn.DataParallel
            if not isinstance(self.online_net, nn.DataParallel):
                self.online_net = nn.DataParallel(self.online_net)
        else:
            # If the model is running on a single GPU, remove the "module." prefix from the keys in the state dictionary
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k[7:]  # remove 'module.' prefix
                new_state_dict[name] = v
            state_dict = new_state_dict

        # Load the state dictionary into the model
        self.online_net.load_state_dict(state_dict)


    def get_state(self, t, asset_df):
        num_rows = t - self.window_size + 1
        if num_rows >= 0:
            window = asset_df.iloc[num_rows : t + 1]
        else:
            repeated_first_row = pd.concat([pd.DataFrame(np.repeat(asset_df.iloc[[0]].values, -num_rows, axis=0), columns=asset_df.columns)])
            new_data = asset_df.iloc[0 : t + 1]
            window = pd.concat([repeated_first_row, new_data], ignore_index=True)  # prevents us from sampling data that doesn't exist at the start.
        return window.values