import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
from time import time
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import os
from collections import OrderedDict

# to run distributed training
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp

from DDDQN import DDDQN
from PrioritizedExpReplay import PrioritizedExpReplay

class Agent():

    # Initializes the agent with various parameters such as window_size, gamma, number of episodes, learning rate, minimum epsilon, and update interval. 
    # It also initializes the device, portfolio, epsilon, training step counter, memory, batch size, online and target networks, optimizer, scheduler, 
    # and the holding time.
    def __init__(self, data_shape, num_episodes, window_size=30, gamma=0.99, update_interval=30, lr=0.001, min_epsilon=0.02, local_rank=0):
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.window_size = window_size  # number of previous prices to use as state
        self.data_shape = data_shape  # [num_features, window_size]
        self.portfolio = [0, 0, 0]  # [total_assets, cash_held, total_portfolio_value (asset value + cash held - initial investment)]
        self.inventory = [] 
        self.gamma = gamma  # discount factor for future rewards
        self.num_episodes = num_episodes    # number of episodes
        self.epsilon = 1.0  # exploration rate
        self.min_epsilon = min_epsilon   # minimum exploration rate
        self.update_interval = update_interval  # number of steps between target network updates
        self.trainstep = 0
        self.memory = PrioritizedExpReplay(self.window_size, data_shape[1], self.device)
        self.batch_size = 32
        self.online_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net = DDDQN(self.data_shape[1], window_size).to(self.device)
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.holding_time = 0 # Number of timesteps for which the agent has held the position
        initial_learning_rate = lr
        decay_steps = self.num_episodes * self.data_shape[0] // 10  # adjust the divisor to control the decay rate
        decay_rate = 0.9  # adjust this value to control the decay rate
        self.optimizer = optim.Adam(self.online_net.parameters(), lr=initial_learning_rate)
        self.scheduler = ExponentialLR(self.optimizer, gamma=decay_rate)
        self.criterion = nn.MSELoss()


        self.local_rank = local_rank
        torch.cuda.set_device(self.local_rank)
        if not dist.is_initialized():
            dist.init_process_group(backend='gloo', init_method='env://')

        if os.path.exists("Output/online_model/model.pt"):
            print("Loading the model")
            self.load_model()
        else:
            print("No model found, training from scratch")

        self.online_net = self.online_net.to(self.device)
        self.target_net = self.target_net.to(self.device)
        self.online_net = DDP(self.online_net, device_ids=[self.local_rank], output_device=self.local_rank)
        self.target_net = DDP(self.target_net, device_ids=[self.local_rank], output_device=self.local_rank)

    # CURRICULUM TRAINING PART 1
    def get_action(self, state, t, reference_df):
        state_numpy = np.array(state)
        state_tensor = torch.FloatTensor(state_numpy).to(self.device).unsqueeze(0)
        if reference_df['timestamp'].iloc[t] <= reference_df['FILING_DATE'].iloc[t]:  # if the current time is before the filing date, we can't trade
            return 1  # hold
        elif np.random.rand() <= self.epsilon:
            if len(self.inventory) > 0:  # if we have inventory to sell
                return np.random.choice([0, 1, 2]) # we can sell, hold, buy
            else:
                return np.random.choice([1, 2])  # we can only buy or hold
        else:
            with torch.no_grad():
                self.online_net.eval()
                actions = self.online_net(state_tensor)
                self.online_net.train()

                if len(self.inventory) > 0:
                    action = torch.argmax(actions[0, :]).item()
                else: # if we don't have inventory to sell, we remove that action option
                    action = torch.argmax(actions[0, 1:]).item() + 1
                return action

    #  This method softly updates the target network by blending the weights of the online network and target network using the parameter tau.
    def update_target(self, tau=0.001):
        for target_param, online_param in zip(self.target_net.parameters(), self.online_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)

    #  Linearly decays the epsilon value from its initial value to the minimum epsilon value over the course of the total training steps.
    def update_epsilon(self):
        if self.epsilon > self.min_epsilon:
            # b = self.min_epsilon**(1/(self.num_episodes*self.data_shape[0]))
            # self.epsilon = b**self.trainstep # exponential decay
            self.epsilon -= (1.0 - self.min_epsilon) / (self.num_episodes * self.data_shape[0])   # linear decay

    # Trains the online network using the samples from the prioritized experience replay memory. 
    # It updates the target network periodically based on the update_interval. 
    # It also calculates the loss, applies the importance sampling weights, and updates the priorities in the buffer.
    def train(self):
        if self.memory.tree.n_entries < self.batch_size:
            return

        if self.trainstep % self.update_interval == 0:
            self.update_target()

        states, actions, rewards, next_states, dones, idxs, is_weights = self.memory.sample_exp(self.batch_size)

        q_next_state_online_net = self.online_net(next_states)
        q_next_state_target_net = self.target_net(next_states)

        max_action = torch.argmax(q_next_state_online_net, dim=1).to(self.device)

        batch_index = torch.arange(self.batch_size, dtype=torch.int64).to(self.device)

        q_predicted = self.online_net(states)
        q_target = q_predicted.clone().detach()

        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action] * dones

        td_errors = q_target - q_predicted
        td_errors = td_errors.detach().cpu().numpy()
        td_errors = td_errors[np.arange(self.batch_size), actions.cpu().numpy()]

        # Update priorities in the buffer
        self.memory.update_priorities(idxs, td_errors)

        loss = self.criterion(q_predicted, q_target).view(-1, 1)
        loss = (loss * is_weights).mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.update_epsilon()
        self.trainstep += 1
        return loss.item()

    def save_model(self):

        output_directory = "Output"
        online_model_directory = os.path.join(output_directory, "online_model")
        target_model_directory = os.path.join(output_directory, "target_model")

        os.makedirs(online_model_directory, exist_ok=True)
        os.makedirs(target_model_directory, exist_ok=True)

        # If the model is wrapped with DataParallel, save the state_dict of the module
        if isinstance(self.online_net, nn.DataParallel):
            torch.save(self.online_net.module.state_dict(), os.path.join(online_model_directory, 'model.pt'))
        else:
            torch.save(self.online_net.state_dict(), os.path.join(online_model_directory, 'model.pt'))

        if isinstance(self.target_net, nn.DataParallel):
            torch.save(self.target_net.module.state_dict(), os.path.join(target_model_directory, 'model.pt'))
        else:
            torch.save(self.target_net.state_dict(), os.path.join(target_model_directory, 'model.pt'))

    def load_model(self):
        # Load the state dictionary from the file
        state_dict = torch.load('Output/online_model/model.pt')

        # Check if the model is running on multiple GPUs or a single GPU
        if torch.cuda.device_count() > 1:
            # If the model is running on multiple GPUs, wrap the model with nn.DataParallel
            if not isinstance(self.online_net, nn.DataParallel):
                self.online_net = nn.DataParallel(self.online_net)
        else:
            # If the model is running on a single GPU, remove the "module." prefix from the keys in the state dictionary
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k[7:]  # remove 'module.' prefix
                new_state_dict[name] = v
            state_dict = new_state_dict

        # Load the state dictionary into the model
        self.online_net.load_state_dict(state_dict)

    
    def trade(self, t, action, asset_df, asset_price_unscaled, initial_investment, trading_fee_rate):
        assets_held, balance, previous_portfolio_value = self.portfolio
        reward = 0
        if action == 0:  # sell
            # reward = asset_price_df_unscaled["close"].iloc[t] - self.inventory.pop(0) # this sells the oldest bought eth
            reward = asset_price_unscaled["close"].iloc[t] - self.inventory.pop() # this sells the most recently bought eth (worth a shot)
        elif action == 1:  # hold
            reward = -0.1
            pass
        elif 2 and balance >= asset_price_unscaled["close"].iloc[t]:  # buy
            self.inventory.append(asset_price_unscaled["close"].iloc[t])
        return reward

    def get_state(self, t, asset_df):
        num_rows = t - self.window_size + 1
        if num_rows >= 0:
            window = asset_df.iloc[num_rows : t + 1]
        else:
            repeated_first_row = pd.concat([pd.DataFrame(np.repeat(asset_df.iloc[[0]].values, -num_rows, axis=0), columns=asset_df.columns)])
            new_data = asset_df.iloc[0 : t + 1]
            window = pd.concat([repeated_first_row, new_data], ignore_index=True)  # prevents us from sampling data that doesn't exist at the start.
        return window.values