{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35868\\3955130631.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Torch imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ericb\\anaconda3\\envs\\ML_Torch\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwith_load_library_flags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadLibraryExW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0x00001100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mlast_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlast_error\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from time import time\n",
    "import argparse\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import classes\n",
    "from Agent import Agent\n",
    "from CryptoData import CryptoData\n",
    "from DataVisualizer import DataVisualizer\n",
    "\n",
    "def setup_distributed():\n",
    "    # Parse command line arguments for 'rank' and 'world_size'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", type=int)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(backend='nccl', init_method='env://')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "\n",
    "\n",
    "class TradingSystem:\n",
    "    def __init__(self, initial_investment=10000, num_episodes=10, local_rank=0):\n",
    "        self.local_rank = local_rank\n",
    "        self.INITIAL_INVESTMENT = initial_investment\n",
    "        self.NUM_EPISODES = num_episodes\n",
    "        self.trading_agent = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.train_close = None\n",
    "        self.test_close = None\n",
    "        self.data_visualizer = None\n",
    "\n",
    "    def manual_data_partition(self, data):\n",
    "        total_size = len(data)\n",
    "        per_gpu = total_size // dist.get_world_size()\n",
    "        start = self.local_rank * per_gpu\n",
    "        end = start + per_gpu if self.local_rank < dist.get_world_size() - 1 else total_size\n",
    "        return data[start:end]\n",
    "\n",
    "    def load_data(self):\n",
    "        crypto_data = CryptoData()\n",
    "        full_train_df, full_test_df, train_close, test_close = crypto_data.get_precollected_data(split_ratio=0.8)\n",
    "\n",
    "        # Partition data for distributed training\n",
    "        self.train_df = self.manual_data_partition(full_train_df)\n",
    "        self.test_df = self.manual_data_partition(full_test_df)\n",
    "        self.train_close = self.manual_data_partition(train_close)\n",
    "        self.test_close = self.manual_data_partition(test_close)\n",
    "\n",
    "    def setup_trading_agent(self):\n",
    "        self.trading_agent = Agent(self.train_df.shape, self.NUM_EPISODES, self.window_size) \n",
    "        if os.path.exists(\"Output/online_model/model.pt\"):\n",
    "            print(\"Loading the model\")\n",
    "            self.trading_agent.load_model()\n",
    "        else:\n",
    "            print(\"No model found, training from scratch\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.trading_agent.model = self.trading_agent.model.cuda(self.local_rank)\n",
    "        self.trading_agent.model = DDP(self.trading_agent.model, device_ids=[self.local_rank])\n",
    "\n",
    "    # Curriculum 1 \n",
    "    def train(self):\n",
    "        episode_mem = [{\"Actions\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Realized Profit\": [], \"Reward\": [], \"Done\": [], \"Epsilon\": [], \"MSE Loss\": []} for _ in range(self.NUM_EPISODES)]\n",
    "        t0 = time()\n",
    "        for s in range(self.NUM_EPISODES):\n",
    "            print(f\"\\n===== Episode {s + 1} / {self.NUM_EPISODES} =====\")\n",
    "            self.trading_agent.inventory = []\n",
    "            state = self.trading_agent.get_state(0, self.train_df)\n",
    "            balance = self.INITIAL_INVESTMENT\n",
    "            portfolio_value_usd = 0\n",
    "            self.trading_agent.portfolio = [0, self.INITIAL_INVESTMENT, portfolio_value_usd, 0]\n",
    "            done = False\n",
    "            for t in range(len(self.train_df) - 1):\n",
    "                if done:\n",
    "                    break\n",
    "                action = self.trading_agent.get_action(state)\n",
    "                next_state = self.trading_agent.get_state(t + 1, self.train_df)\n",
    "                reward = self.trading_agent.trade(t, action, self.train_df, self.train_close, self.INITIAL_INVESTMENT, trading_fee_rate=0.05)\n",
    "                balance += reward\n",
    "                done = balance < self.train_close[\"Close\"].iloc[t]\n",
    "                self.trading_agent.memory.add_exp(state, action, reward, next_state, done)\n",
    "                loss = self.trading_agent.train() or 0\n",
    "                state = next_state\n",
    "                \n",
    "                # Corrected dictionary update part\n",
    "                episode_mem[s][\"Actions\"].append(int(action))\n",
    "                episode_mem[s][\"Inventory Size\"].append(len(self.trading_agent.inventory))\n",
    "                episode_mem[s][\"Portfolio Value\"].append(balance + self.train_close[\"Close\"].iloc[t] * len(self.trading_agent.inventory) - sum(self.trading_agent.inventory) - self.INITIAL_INVESTMENT)\n",
    "                episode_mem[s][\"Realized Profit\"].append(balance - self.INITIAL_INVESTMENT)\n",
    "                episode_mem[s][\"Reward\"].append(reward)\n",
    "                episode_mem[s][\"Done\"].append(done)\n",
    "                episode_mem[s][\"Epsilon\"].append(self.trading_agent.epsilon)\n",
    "                episode_mem[s][\"MSE Loss\"].append(loss)\n",
    "\n",
    "        with open('Output/training_scores.out', 'a') as f:\n",
    "            \n",
    "            f.write(f\"EPISODE {s} (runtime: {time() - t0})   | Portfolio Value is {round(episode_mem[s]['Portfolio Value'][-1], 3)} Epsilon is {round(self.trading_agent.epsilon, 3)}   |   MSE Loss is {round(episode_mem[s]['MSE Loss'][-1], 3)}\\n\")\n",
    "        with open(\"Output/episode_mem.json\", 'w') as f:\n",
    "            json.dump(episode_mem, f)\n",
    "\n",
    "    def test(self):\n",
    "        testing_mem = {\"Actions\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Realized Profit\": [], \"Reward\": [], \"Done\": []}\n",
    "        t0 = time()\n",
    "        self.trading_agent.epsilon = 0\n",
    "        self.trading_agent.inventory = []\n",
    "        state = self.trading_agent.get_state(0, self.test_df)\n",
    "        balance = self.INITIAL_INVESTMENT\n",
    "        portfolio_value_usd = 0\n",
    "        self.trading_agent.portfolio = [0, self.INITIAL_INVESTMENT, portfolio_value_usd, 0]\n",
    "\n",
    "        done = False\n",
    "        for t in range(len(self.test_df) - 1):\n",
    "            if done:\n",
    "                print(\"Done with testing\")\n",
    "                break\n",
    "            action = self.trading_agent.get_action(state)\n",
    "            next_state = self.trading_agent.get_state(t + 1, self.test_df)\n",
    "            reward = self.trading_agent.trade(t, action, self.test_df, self.test_close, self.INITIAL_INVESTMENT, trading_fee_rate=0.05)\n",
    "            balance += reward\n",
    "            done = balance < self.test_close[\"Close\"].iloc[t]\n",
    "            state = next_state\n",
    "            testing_mem.update({\n",
    "                \"Actions\": int(action),\n",
    "                \"Inventory Size\": len(self.trading_agent.inventory),\n",
    "                \"Portfolio Value\": float(balance + self.test_close[\"Close\"].iloc[t] * len(self.trading_agent.inventory) - sum(self.trading_agent.inventory)) - self.INITIAL_INVESTMENT,\n",
    "                \"Realized Profit\": float(balance - self.INITIAL_INVESTMENT),\n",
    "                \"Reward\": float(reward),\n",
    "                \"Done\": bool(done)\n",
    "            })\n",
    "\n",
    "        if dist.rank == 0:\n",
    "            with open('Output/testing_scores.out', 'a') as f:\n",
    "                f.write(f\"TESTING (runtime: {time() - t0})   |  Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\")\n",
    "            with open(\"Output/testing_mem.json\", 'w') as f:\n",
    "                json.dump(testing_mem, f)\n",
    "\n",
    "    def run(self):\n",
    "        print(\"PyTorch version \" + torch.__version__)\n",
    "        print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "        # grab the gpu id if available\n",
    "        print(\"GPU available: \", torch.cuda.is_available())\n",
    "        print(\"GPU device: \", torch.cuda.get_device_name(0))\n",
    "        print(\"GPU device count: \", torch.cuda.device_count())\n",
    "        print(\"GPU device index: \", torch.cuda.current_device())\n",
    "\n",
    "        print(torch.cuda.is_available())\n",
    "\n",
    "        setup_distributed()\n",
    "\n",
    "        self.load_and_prepare_data()\n",
    "        self.setup_trading_agent()\n",
    "        self.train()\n",
    "        self.test()\n",
    "\n",
    "        # Only save models or log information from one process to avoid conflicts\n",
    "        if dist.get_rank() == 0:\n",
    "            self.trading_agent.save_model()\n",
    "            print(\"Model saved\")\n",
    "\n",
    "        self.data_visualizer = DataVisualizer(self.train_close, self.test_close)\n",
    "        self.data_visualizer.visualize_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = setup_distributed()\n",
    "    trading_system = TradingSystem(local_rank=args.local_rank)  # Pass local_rank to TradingSystem\n",
    "    trading_system.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - edit this main to work with the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(trading_agent, train_df, test_df, train_close, test_close, curriculum):\n",
    "\n",
    "    CURRICULUM = curriculum\n",
    "    \n",
    "    # CURRICULUM 1\n",
    "    if CURRICULUM == 1:\n",
    "        print(\"Curriculum 1\")\n",
    "        episode_mem = curriculum_1_episode_memory(NUM_EPISODES=NUM_EPISODES)\n",
    "    # CURRICULUM 2\n",
    "    elif CURRICULUM == 2:\n",
    "        episode_mem = curriculum_2_episode_memory(NUM_EPISODES=NUM_EPISODES)\n",
    "\n",
    "    t0 = time()  # start time of the program\n",
    "\n",
    "    ######################## Training ########################\n",
    "    for episode in range(NUM_EPISODES):  \n",
    "        print(f\"\\n===== Episode {episode + 1} / {NUM_EPISODES} =====\")\n",
    "        trading_agent.inventory = []\n",
    "        state = trading_agent.get_state(0, train_df)\n",
    "        balance = INITIAL_INVESTMENT\n",
    "        if CURRICULUM == 1:\n",
    "            portfolio_value_usd = 0\n",
    "        if CURRICULUM == 2:\n",
    "            portfolio_value_usd = INITIAL_INVESTMENT\n",
    "        # Reset the agent's portfolio at the beginning of each episode\n",
    "        trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd, 0]\n",
    "\n",
    "        done = False\n",
    "        for t in range(len(train_df) - 1):\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # CURRICULUM 1\n",
    "            if CURRICULUM == 1:\n",
    "                action, next_state, reward, done, loss, new_balance = curriculum_1_train(INITIAL_INVESTMENT, t, trading_agent, train_df, train_close, state, balance)\n",
    "                balance = new_balance\n",
    "                portfolio_value_usd = float(balance + train_close[\"Close\"].iloc[t] * len(trading_agent.inventory) - sum(trading_agent.inventory)) - INITIAL_INVESTMENT\n",
    "                if not loss:\n",
    "                    loss = 0\n",
    "                    state = next_state\n",
    "\n",
    "                episode_mem[episode][\"Actions\"].append(int(action))\n",
    "                episode_mem[episode][\"Inventory Size\"].append(len(trading_agent.inventory))\n",
    "                episode_mem[episode][\"Portfolio Value\"].append(portfolio_value_usd)\n",
    "                episode_mem[episode][\"Realized Profit\"].append(float(balance - INITIAL_INVESTMENT))\n",
    "                episode_mem[episode][\"Reward\"].append(float(reward))\n",
    "                episode_mem[episode][\"Done\"].append(bool(done))\n",
    "                episode_mem[episode][\"Epsilon\"].append(trading_agent.epsilon)\n",
    "                episode_mem[episode][\"MSE Loss\"].append(loss)\n",
    "\n",
    "                if t % 100 == 0:\n",
    "                    print(f\"Time step {t} / {len(train_df)}   |   Portfolio Value: {round(episode_mem[episode]['Portfolio Value'][t], 3)}   |   Inventory: {len(trading_agent.inventory)}   |   ETH$: {round(train_close['Close'].iloc[t], 3)}   |   Epsilon: {round(trading_agent.epsilon, 4)}   |   MSE Loss: {loss}\")\n",
    "\n",
    "\n",
    "            \n",
    "            # CURRICULUM 2\n",
    "            elif CURRICULUM == 2:\n",
    "                action, next_state, reward, done, loss, new_balance, new_portfolio_value = curriculum_2_train(INITIAL_INVESTMENT, t, trading_agent, train_df, train_close, state, balance)\n",
    "                balance = new_balance\n",
    "                portfolio_value_usd = new_portfolio_value\n",
    "                eth_held, cash_held, new_portfolio_value, max_ether_held_so_far = trading_agent.portfolio\n",
    "\n",
    "                if not loss:\n",
    "                    loss = 0\n",
    "                    state = next_state\n",
    "\n",
    "                episode_mem[episode][\"Actions\"].append(int(action))\n",
    "                episode_mem[episode][\"Eth Held\"].append(float(eth_held))\n",
    "                episode_mem[episode][\"Cash Held\"].append(round(float(balance), 2))\n",
    "                episode_mem[episode][\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "                episode_mem[episode][\"Reward\"].append(float(reward))\n",
    "                episode_mem[episode][\"Done\"].append(bool(done))\n",
    "                episode_mem[episode][\"Epsilon\"].append(trading_agent.epsilon)\n",
    "                episode_mem[episode][\"MSE Loss\"].append(float(loss))\n",
    "\n",
    "                if t % 1 == 0:\n",
    "                    print(f\"Time step {t} / {len(train_df)}   |  Eth: {round(episode_mem[episode]['Eth Held'][t], 7)}  |  Cash: {round(episode_mem[episode]['Cash Held'][t], 2)}  |  Portfolio Value: {round(episode_mem[episode]['Portfolio Value'][t], 3)}  |   MSE: {round(episode_mem[episode]['MSE Loss'][t], 3)}  |  Reward: {round(episode_mem[episode]['Reward'][t], 3)}  |  Action: {int(episode_mem[episode]['Actions'][t])}  |  Epsilon: {round(trading_agent.epsilon, 3)}\")\n",
    "            \n",
    "    if CURRICULUM == 1:\n",
    "        # if we have leftover inventory at the end of the episode, calculate its value by selling them all at the current market rate\n",
    "        balance += train_df[\"Close\"].iloc[t] * len(trading_agent.inventory) - sum(trading_agent.inventory)\n",
    "    \n",
    "    with open('Output/training_scores.out', 'a') as f:\n",
    "        f.write(f\"EPISODE {episode} (runtime: {time() - t0})   | Portfolio Value is {round(episode_mem[episode]['Portfolio Value'][-1], 3)} Epsilon is {round(trading_agent.epsilon, 3)}   |   MSE Loss is {round(episode_mem[episode]['MSE Loss'][-1], 3)}\\n\")\n",
    "    # Save the episode memory to a json file, if one exists, add a number to the end of the file name\n",
    "    episode_mem_file = \"Output/episode_mem.json\"\n",
    "\n",
    "    with open(episode_mem_file, 'w') as f:\n",
    "        json.dump(episode_mem, f)\n",
    "\n",
    "######################## Testing ########################\n",
    "    t0 = time()\n",
    "    # CURRICULUM 1\n",
    "    if CURRICULUM == 1:\n",
    "        testing_mem = curriculum_1_test_memory()\n",
    "    # CURRICULUM 2\n",
    "    elif CURRICULUM == 2:\n",
    "        testing_mem = curriculum_2_test_memory()\n",
    "\n",
    "    trading_agent.epsilon = 0  # no exploration\n",
    "    if CURRICULUM == 1:\n",
    "        trading_agent.inventory = [] # inventory reset\n",
    "\n",
    "    state = trading_agent.get_state(0, test_df)\n",
    "    balance = INITIAL_INVESTMENT\n",
    "    \n",
    "    if CURRICULUM == 1:\n",
    "        portfolio_value_usd = 0\n",
    "    if CURRICULUM == 2:\n",
    "        portfolio_value_usd = INITIAL_INVESTMENT\n",
    "    \n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "    trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd, 0]\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "\n",
    "    done = False\n",
    "    for t in range(len(test_df) - 1):\n",
    "        if done:\n",
    "            print(\"Done with testing\")\n",
    "            break\n",
    "\n",
    "        # CURRICULUM 1\n",
    "        if CURRICULUM == 1:\n",
    "            action, next_state, reward, done, new_balance = curriculum_1_test(INITIAL_INVESTMENT, t, trading_agent, test_df, test_close, state, balance)\n",
    "            balance = new_balance\n",
    "\n",
    "            # CURRICULUM 1\n",
    "            testing_mem[\"Actions\"].append(int(action))\n",
    "            testing_mem[\"Inventory Size\"].append(len(trading_agent.inventory))\n",
    "            testing_mem[\"Portfolio Value\"].append(float(balance + test_close[\"Close\"].iloc[t] * len(trading_agent.inventory) - sum(trading_agent.inventory)) - INITIAL_INVESTMENT)\n",
    "            testing_mem[\"Realized Profit\"].append(float(balance - INITIAL_INVESTMENT))\n",
    "            testing_mem[\"Reward\"].append(float(reward))\n",
    "            testing_mem[\"Done\"].append(bool(done))\n",
    "\n",
    "            if t % 1 == 0:\n",
    "                print(f\"Time step {t} / {len(test_df)}   |   Inventory Size: {len(trading_agent.inventory)}  |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   |  Action: {int(action)}  |  Reward: {round(reward, 3)}\")\n",
    "\n",
    "        # CURRICULUM 2\n",
    "        elif CURRICULUM == 2:\n",
    "            action, next_state, reward, done, loss, new_balance, new_portfolio_value = curriculum_2_test(INITIAL_INVESTMENT, t, trading_agent, test_df, test_close, state, balance)\n",
    "            balance = new_balance\n",
    "            portfolio_value_usd = new_portfolio_value\n",
    "            eth_held, cash_held, new_portfolio_value, max_ether_held_so_far = trading_agent.portfolio\n",
    "\n",
    "            # CURRICULUM 2\n",
    "            testing_mem[\"Actions\"].append(int(action))\n",
    "            testing_mem[\"Eth Held\"].append(float(eth_held))\n",
    "            testing_mem[\"Cash Held\"].append(round(float(balance),2))\n",
    "            testing_mem[\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "            testing_mem[\"Reward\"].append(float(reward))\n",
    "            testing_mem[\"Done\"].append(bool(done))\n",
    "\n",
    "            if t % 1 == 0:\n",
    "                print(f\"Time step {t} / {len(test_df)}   |   Eth Held: {round(testing_mem['Eth Held'][t], 7)}  |  Cash Held: {round(testing_mem['Cash Held'][t], 2)}  |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   |  Action: {int(action)}  |  Reward: {round(reward, 3)}\")\n",
    "    \n",
    "    if CURRICULUM == 1:\n",
    "        # if we have leftover inventory at the end of the episode, calculate its value by selling them all at the current market rate\n",
    "        balance += test_close[\"Close\"].iloc[t] * len(trading_agent.inventory) - sum(trading_agent.inventory)\n",
    "\n",
    "    with open('Output/testing_scores.out', 'a') as f:\n",
    "        f.write(f\"TESTING (runtime: {time() - t0})   |  Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\")\n",
    "\n",
    "    # save the testing memory to a json file\n",
    "    testing_mem_file = \"Output/testing_mem.json\"\n",
    "    with open(testing_mem_file, 'w') as f:\n",
    "        json.dump(testing_mem, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    INITIAL_INVESTMENT = 10000\n",
    "    NUM_EPISODES = 1000\n",
    "    START_DATE = \"2017-01-01-00-00\"\n",
    "    END_DATE = \"2020-01-01-00-00\"\n",
    "    TESTING_SPLIT = \"2022-01-01-00:00:00\" # formatting is different here because of how historic_crypto indexes dataframes\n",
    "    CURRICULUM = 1\n",
    "\n",
    "    train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "    trading_agent = Agent(train_df.shape, NUM_EPISODES)\n",
    "    \n",
    "    # Load the model only once before starting the loop if it exists\n",
    "    if os.path.exists(\"Output/online_model/model.pt\"):\n",
    "        print(\"Loading the model\")\n",
    "        trading_agent.load_model()\n",
    "    else:\n",
    "        print(\"No model found, training from scratch\")\n",
    "\n",
    "    main(trading_agent, train_df, test_df, train_close, test_close, CURRICULUM)\n",
    "    trading_agent.save_model()\n",
    "    print(\"model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_AI_RL_20230410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
