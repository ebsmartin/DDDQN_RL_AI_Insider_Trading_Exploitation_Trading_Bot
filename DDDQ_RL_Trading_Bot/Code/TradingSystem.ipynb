{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.13.1\n",
      "Num GPUs Available:  1\n",
      "GPU available:  True\n",
      "GPU device:  NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "GPU device count:  1\n",
      "GPU device index:  0\n",
      "True\n",
      "sorted_storm_output_2022.csv\n",
      "No model found, training from scratch\n",
      "\n",
      "===== Episode 1 : 0000920112-22-000115 =====\n",
      "\n",
      "                            Time step 0 / 650   \n",
      "                            |   Portfolio Value: -0.1   \n",
      "                            |   Inventory: 0   \n",
      "                            |   ETH$: 45.061   \n",
      "                            |   Epsilon: 1.0   \n",
      "                            |   MSE Loss: 0\n",
      "                            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\Desktop\\CS 535 - Big Data\\Term_Projcet\\DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot\\DDDQ_RL_Trading_Bot\\Code\\PrioritizedExpReplay.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Range exceeds valid bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31332\\3863407273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\ericb\\\\Desktop\\\\CS 535 - Big Data\\\\Term_Projcet\\\\Storm_ouput\\\\Data_Input\\\\\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;31m# Run the trading system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m     \u001b[0mtrading_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31332\\3863407273.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_and_prepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31332\\3863407273.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbalance\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mreference_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"close\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrading_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrading_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ericb\\Desktop\\CS 535 - Big Data\\Term_Projcet\\DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot\\DDDQ_RL_Trading_Bot\\Code\\Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mq_next_state_online_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ericb\\Desktop\\CS 535 - Big Data\\Term_Projcet\\DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot\\DDDQ_RL_Trading_Bot\\Code\\PrioritizedExpReplay.py\u001b[0m in \u001b[0;36msample_exp\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mpriorities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Range exceeds valid bounds"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import argparse\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import classes\n",
    "from Agent import Agent\n",
    "from DataVisualizer import DataVisualizer\n",
    "from DataManager import DataManager\n",
    "\n",
    "# set Rank environment variables\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "def setup_distributed(local_rank):\n",
    "    # Set environment variables\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['RANK'] = '0'\n",
    "\n",
    "    # Initialize the process group\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(backend='gloo', init_method='env://')\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    return local_rank\n",
    "\n",
    "\n",
    "class TradingSystem:\n",
    "    def __init__(self, initial_investment=10000, num_episodes=10, local_rank=0):\n",
    "        self.local_rank = local_rank\n",
    "        self.INITIAL_INVESTMENT = initial_investment\n",
    "        self.NUM_EPISODES = num_episodes\n",
    "        self.trading_agent = None\n",
    "        self.data_manager = DataManager()\n",
    "        self.train_df_shape = None\n",
    "        self.test_df_shape = None\n",
    "        self.data_visualizer = None\n",
    "\n",
    "    def load_and_prepare_data(self, directory):\n",
    "        # create the test and train df dictionaries for each csv file in the directory\n",
    "        self.data_manager.prepare_data(directory)\n",
    "        self.NUM_EPISODES = len(self.data_manager.train_data_dict)\n",
    "\n",
    "    def manual_data_partition(self, data):\n",
    "        total_size = len(data)\n",
    "        per_gpu = total_size // dist.get_world_size()\n",
    "        start = self.local_rank * per_gpu\n",
    "        end = start + per_gpu if self.local_rank < dist.get_world_size() - 1 else total_size\n",
    "        return data[start:end]\n",
    "\n",
    "    def setup_trading_agent(self):\n",
    "        self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES) \n",
    "        if os.path.exists(\"Output/online_model/model.pt\"):\n",
    "            print(\"Loading the model\")\n",
    "            self.trading_agent.load_model()\n",
    "        else:\n",
    "            print(\"No model found, training from scratch\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.trading_agent.model = self.trading_agent.model.cuda(self.local_rank)\n",
    "        self.trading_agent.model = DDP(self.trading_agent.model, device_ids=[self.local_rank])\n",
    "\n",
    "    # Curriculum 1 \n",
    "    def train(self):\n",
    "        episode_mem = [{\"Actions\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Realized Profit\": [], \"Reward\": [], \"Done\": [], \"Epsilon\": [], \"MSE Loss\": []} for _ in range(self.NUM_EPISODES)]\n",
    "        t0 = time()\n",
    "        episode_num = 0\n",
    "        for accession_number, data in self.data_manager.train_data_dict.items():\n",
    "            normalized_df, reference_df = data\n",
    "            self.train_df_shape = normalized_df.shape\n",
    "            self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES)\n",
    "            print(f\"\\n===== Episode {episode_num + 1} : {accession_number} =====\")\n",
    "            self.trading_agent.inventory = []\n",
    "            state = self.trading_agent.get_state(0, normalized_df)\n",
    "            balance = self.INITIAL_INVESTMENT\n",
    "            portfolio_value_usd = 0\n",
    "            self.trading_agent.portfolio = [0, self.INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "            done = False\n",
    "            for t in range(len(normalized_df) - 1):\n",
    "                if done:\n",
    "                    break\n",
    "                action = self.trading_agent.get_action(state)\n",
    "                next_state = self.trading_agent.get_state(t + 1, normalized_df)\n",
    "                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, self.INITIAL_INVESTMENT, trading_fee_rate=0.05)\n",
    "                balance += reward\n",
    "                done = balance < reference_df[\"close\"].iloc[t]\n",
    "                self.trading_agent.memory.add_exp(state, action, reward, next_state, done)\n",
    "                loss = self.trading_agent.train() or 0\n",
    "                state = next_state\n",
    "\n",
    "                episode_mem[episode_num][\"Actions\"].append(int(action))\n",
    "                episode_mem[episode_num][\"Inventory Size\"].append(len(self.trading_agent.inventory))\n",
    "                episode_mem[episode_num][\"Portfolio Value\"].append(balance + reference_df[\"close\"].iloc[t] * len(self.trading_agent.inventory) - sum(self.trading_agent.inventory) - self.INITIAL_INVESTMENT)\n",
    "                episode_mem[episode_num][\"Realized Profit\"].append(balance - self.INITIAL_INVESTMENT)\n",
    "                episode_mem[episode_num][\"Reward\"].append(reward)\n",
    "                episode_mem[episode_num][\"Done\"].append(done)\n",
    "                episode_mem[episode_num][\"Epsilon\"].append(self.trading_agent.epsilon)\n",
    "                episode_mem[episode_num][\"MSE Loss\"].append(loss)\n",
    "\n",
    "                if t % 100 == 0:\n",
    "                    print(f\"\"\"\n",
    "                            Time step {t} / {len(normalized_df)}   \n",
    "                            |   Portfolio Value: {round(episode_mem[episode_num]['Portfolio Value'][t], 3)}   \n",
    "                            |   Inventory: {len(self.trading_agent.inventory)}   \n",
    "                            |   {reference_df['ISSUERTRADINGSYMBOL']}: ${round(reference_df['close'].iloc[t], 3)}   \n",
    "                            |   Epsilon: {round(self.trading_agent.epsilon, 4)}   \n",
    "                            |   MSE Loss: {loss}\n",
    "                            \"\"\")\n",
    "            episode_num += 1\n",
    "\n",
    "        with open('Output/training_scores.out', 'a') as f:\n",
    "            f.write(f\"\"\"\n",
    "                    EPISODE {episode_num + 1} : {accession_number} (runtime: {time() - t0})   \n",
    "                    | Portfolio Value is {round(episode_mem[episode_num]['Portfolio Value'][-1], 3)} \n",
    "                    Epsilon is {round(self.trading_agent.epsilon, 3)}   |   \n",
    "                    MSE Loss is {round(episode_mem[episode_num]['MSE Loss'][-1], 3)}\\n\n",
    "                    \"\"\")\n",
    "        with open(\"Output/episode_mem.json\", 'w') as f:\n",
    "            json.dump(episode_mem, f)\n",
    "\n",
    "    def test(self):\n",
    "        testing_mem = {\"Actions\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Realized Profit\": [], \n",
    "                        \"Reward\": [], \"Done\": []}\n",
    "        t0 = time()\n",
    "        self.NUM_EPISODES = len(self.data_manager.test_data_dict)\n",
    "        \n",
    "        for accession_number, data in self.data_manager.test_data_dict.items():\n",
    "            normalized_df, reference_df = data\n",
    "            self.test_df_shape = normalized_df.shape\n",
    "            self.trading_agent = Agent(self.test_df_shape, self.NUM_EPISODES)\n",
    "            self.trading_agent.epsilon = 0\n",
    "            self.trading_agent.inventory = []\n",
    "            state = self.trading_agent.get_state(0, normalized_df)\n",
    "            balance = self.INITIAL_INVESTMENT\n",
    "            portfolio_value_usd = 0\n",
    "            self.trading_agent.portfolio = [0, self.INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "\n",
    "            done = False\n",
    "            for t in range(len(normalized_df) - 1):\n",
    "                if done:\n",
    "                    print(\"Done with testing\")\n",
    "                    break\n",
    "                action = self.trading_agent.get_action(state)\n",
    "                next_state = self.trading_agent.get_state(t + 1, normalized_df)\n",
    "                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, self.INITIAL_INVESTMENT, trading_fee_rate=0.05)\n",
    "                balance += reward\n",
    "                done = balance < reference_df[\"close\"].iloc[t]\n",
    "                state = next_state\n",
    "                testing_mem.update({\n",
    "                    \"Actions\": int(action),\n",
    "                    \"Inventory Size\": len(self.trading_agent.inventory),\n",
    "                    \"Portfolio Value\": float(balance + reference_df[\"close\"].iloc[t] * len(self.trading_agent.inventory) - sum(self.trading_agent.inventory)) - self.INITIAL_INVESTMENT,\n",
    "                    \"Realized Profit\": float(balance - self.INITIAL_INVESTMENT),\n",
    "                    \"Reward\": float(reward),\n",
    "                    \"Done\": bool(done)\n",
    "                })\n",
    "                if t % 1 == 0:\n",
    "                    print(f\"\"\"\n",
    "                            Time step {t} / {len(normalized_df)}   \n",
    "                            |   Inventory Size: {len(self.trading_agent.inventory)}  \n",
    "                            |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   \n",
    "                            |  Action: {int(action)}  |  Reward: {round(reward, 3)}\n",
    "                            \"\"\")\n",
    "\n",
    "            if dist.rank == 0:\n",
    "                with open('Output/testing_scores.out', 'a') as f:\n",
    "                    f.write(f\"\"\"TESTING (runtime: {time() - t0})   |  \n",
    "                            Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\n",
    "                            \"\"\")\n",
    "                with open(\"Output/testing_mem.json\", 'w') as f:\n",
    "                    json.dump(testing_mem, f)\n",
    "\n",
    "\n",
    "    def run(self, directory):\n",
    "        print(\"PyTorch version \" + torch.__version__)\n",
    "        print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "        # grab the gpu id if available\n",
    "        print(\"GPU available: \", torch.cuda.is_available())\n",
    "        print(\"GPU device: \", torch.cuda.get_device_name(0))\n",
    "        print(\"GPU device count: \", torch.cuda.device_count())\n",
    "        print(\"GPU device index: \", torch.cuda.current_device())\n",
    "\n",
    "        print(torch.cuda.is_available())\n",
    "\n",
    "        self.local_rank = setup_distributed(self.local_rank)\n",
    "\n",
    "        self.load_and_prepare_data(directory)\n",
    "        self.train()\n",
    "        self.test()\n",
    "\n",
    "        # Only save models or log information from one process to avoid conflicts\n",
    "        if dist.get_rank() == 0:\n",
    "            self.trading_agent.save_model()\n",
    "            print(\"Model saved\")\n",
    "\n",
    "        # self.data_visualizer = DataVisualizer(self.train_reference, self.test_reference)\n",
    "        # self.data_visualizer.visualize_data()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the distributed environment\n",
    "    local_rank = 0\n",
    "\n",
    "    # Create a TradingSystem instance\n",
    "    trading_system = TradingSystem(initial_investment=10000, num_episodes=10, local_rank=local_rank)\n",
    "\n",
    "    directory = \"C:\\\\Users\\\\ericb\\\\Desktop\\\\CS 535 - Big Data\\\\Term_Projcet\\\\Storm_ouput\\\\Data_Input\\\\\"\n",
    "    # Run the trading system\n",
    "    trading_system.run(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9/1/2022 9:30\n",
       "1    9/1/2022 10:00\n",
       "2    9/1/2022 10:30\n",
       "3    9/1/2022 11:00\n",
       "4    9/1/2022 11:30\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ericb\\\\Desktop\\\\CS 535 - Big Data\\\\Term_Projcet\\\\Storm_ouput\\\\Data_Input\\\\sorted_storm_output_2022.csv\")\n",
    "# print first 5 rows of timestamp col\n",
    "df['timestamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_AI_RL_20230410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
