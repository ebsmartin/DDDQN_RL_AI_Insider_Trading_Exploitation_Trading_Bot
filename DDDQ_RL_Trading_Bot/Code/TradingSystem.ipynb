{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, get_rank, get_world_size, destroy_process_group\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import classes\n",
    "from Agent import Agent\n",
    "from DataVisualizer import DataVisualizer\n",
    "from DataManager import DataManager\n",
    "from NumpyEncoder import NumpyEncoder\n",
    "\n",
    "# set Rank environment variables\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "def setup_distributed(local_rank):\n",
    "    # Set environment variables\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['RANK'] = '0'\n",
    "\n",
    "    # Initialize the process group\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(backend='gloo', init_method='env://')\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    return local_rank\n",
    "\n",
    "\n",
    "class TradingSystem:\n",
    "    def __init__(self, local_rank=0):\n",
    "        self.local_rank = local_rank\n",
    "        self.NUM_EPISODES = None\n",
    "        self.trading_agent = None\n",
    "        self.data_manager = DataManager()\n",
    "        self.train_df_shape = None\n",
    "        self.test_df_shape = None\n",
    "        self.data_visualizer = None\n",
    "\n",
    "    def load_and_prepare_data(self, directory):\n",
    "        # create the test and train df dictionaries for each csv file in the directory\n",
    "        self.data_manager.prepare_data(directory)\n",
    "        self.NUM_EPISODES = len(self.data_manager.train_data_dict)\n",
    "\n",
    "    def manual_data_partition(self, data):\n",
    "        total_size = len(data)\n",
    "        per_gpu = total_size // dist.get_world_size()\n",
    "        start = self.local_rank * per_gpu\n",
    "        end = start + per_gpu if self.local_rank < dist.get_world_size() - 1 else total_size\n",
    "        return data[start:end]\n",
    "\n",
    "    def setup_trading_agent(self):\n",
    "        self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES) \n",
    "        if os.path.exists(\"Output/online_model/model.pt\"):\n",
    "            print(\"Loading the model\")\n",
    "            self.trading_agent.load_model()\n",
    "        else:\n",
    "            print(\"No model found, training from scratch\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.trading_agent.model = self.trading_agent.model.cuda(self.local_rank)\n",
    "        self.trading_agent.model = DDP(self.trading_agent.model, device_ids=[self.local_rank])\n",
    "        \n",
    "    # Curriculum 1 \n",
    "    def train(self):\n",
    "        episode_mem = [{\"Actions\": [], \"Purchases\": [], \"Sales\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Balance\": [], \"Reward\": [], \"Epsilon\": [], \"MSE Loss\": []} for _ in range(self.NUM_EPISODES)]\n",
    "        t0 = time()\n",
    "        episode_num = 0\n",
    "        print(f'Episodes total : {self.NUM_EPISODES}')\n",
    "        for accession_number, data in self.data_manager.train_data_dict.items():\n",
    "            normalized_df, reference_df = data\n",
    "            self.train_df_shape = normalized_df.shape\n",
    "            print(f'train_df_shape {self.train_df_shape}')\n",
    "            self.trading_agent = Agent(self.train_df_shape, self.NUM_EPISODES)\n",
    "            print(f\"\\n===== Episode {episode_num + 1} : {accession_number} =====\")\n",
    "            self.trading_agent.inventory = []\n",
    "            state = self.trading_agent.get_state(0, normalized_df)\n",
    "            portfolio_value = 0\n",
    "            balance = 0\n",
    "            self.trading_agent.portfolio = [0, balance, portfolio_value]\n",
    "            print(len(normalized_df))\n",
    "            for t in range(len(normalized_df) - 1):\n",
    "                action = self.trading_agent.get_action(state, t, reference_df)\n",
    "                \n",
    "                if action == 0:\n",
    "                    episode_mem[episode_num][\"Sales\"].append(reference_df['close'].iloc[t])\n",
    "                elif action == 2:\n",
    "                    episode_mem[episode_num][\"Purchases\"].append(reference_df['close'].iloc[t])\n",
    "                \n",
    "                next_state = self.trading_agent.get_state(t + 1, normalized_df)\n",
    "                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, len(normalized_df)-1, trading_fee_rate=0.01)\n",
    "\n",
    "                self.trading_agent.memory.add_exp(state, action, reward, next_state)\n",
    "                loss = self.trading_agent.train() or 0\n",
    "                state = next_state\n",
    "\n",
    "                assets_held, balance, portfolio_value = self.trading_agent.portfolio\n",
    "\n",
    "                episode_mem[episode_num][\"Actions\"].append(int(action))\n",
    "                episode_mem[episode_num][\"Inventory Size\"].append(len(self.trading_agent.inventory))\n",
    "                episode_mem[episode_num][\"Portfolio Value\"].append(portfolio_value)\n",
    "                episode_mem[episode_num][\"Balance\"].append(balance)\n",
    "                episode_mem[episode_num][\"Reward\"].append(reward)\n",
    "                episode_mem[episode_num][\"Epsilon\"].append(self.trading_agent.epsilon)\n",
    "                episode_mem[episode_num][\"MSE Loss\"].append(loss)\n",
    "\n",
    "                # last 10 rewards \n",
    "                \n",
    "                last_3_rewards = episode_mem[episode_num][\"Reward\"][-3:]\n",
    "                if t % 50 == 0:\n",
    "                    print(f\"\"\"\n",
    "                            Time step {t} / {len(normalized_df)}   \n",
    "                            |   Inventory Size: {len(self.trading_agent.inventory)}\n",
    "                            |   Last 3 Actions: {episode_mem[episode_num][\"Actions\"][-3:]}\n",
    "                            |   Portfolio Value: {round(episode_mem[episode_num]['Portfolio Value'][t], 3)} \n",
    "                            |   Balance: {round(episode_mem[episode_num]['Balance'][t], 3)}\n",
    "                            |   {reference_df['ISSUERTRADINGSYMBOL'].iloc[t]}: ${round(reference_df['close'].iloc[t], 3)}   \n",
    "                            |   Epsilon: {round(self.trading_agent.epsilon, 4)}   \n",
    "                            |   MSE Loss: {loss}\n",
    "                            |   Last 3 Rewards: {last_3_rewards}\n",
    "                            |   Average Reward: {np.mean(episode_mem[episode_num][\"Reward\"])}\n",
    "                            |   Max Reward: {max(episode_mem[episode_num][\"Reward\"])}\n",
    "                            |   Min Reward: {min(episode_mem[episode_num][\"Reward\"])}\n",
    "                            \"\"\")\n",
    "            episode_num += 1\n",
    "            self.trading_agent.save_model()\n",
    "\n",
    "        total_sell_actions = sum(episode['Actions'].count(0) for episode in episode_mem)\n",
    "        total_buy_actions = sum(episode['Actions'].count(2) for episode in episode_mem)\n",
    "        total_hold_actions = sum(episode['Actions'].count(1) for episode in episode_mem)\n",
    "        average_reward = np.mean([sum(episode['Reward']) for episode in episode_mem])\n",
    "        average_balance = np.mean([sum(episode['Balance']) for episode in episode_mem])\n",
    "\n",
    "        with open('Output/training_scores.out', 'a') as f:\n",
    "            f.write(f\"\"\"\n",
    "                    EPISODES Completed {episode_num + 1} |  (runtime: {time() - t0})   \n",
    "                    | Total Sell Actions taken: {total_sell_actions}   |\n",
    "                    Total Buy Actions taken: {total_buy_actions}   |\n",
    "                    Total Hold Actions taken: {total_hold_actions}   |\n",
    "                    Average Portfolio Value: {np.mean([sum(episode['Portfolio Value']) for episode in episode_mem])}   |\n",
    "                    Average Balance: {average_balance}   |\n",
    "                    Average Reward: {average_reward}   |\n",
    "                    Epsilon is {round(self.trading_agent.epsilon, 3)}   |   \n",
    "                    MSE Loss is {round(episode_mem[episode_num - 1]['MSE Loss'][-1], 3)}\\n\n",
    "                    \"\"\")\n",
    "        with open(\"Output/episode_mem.json\", 'w') as f:\n",
    "            json.dump(episode_mem, f, cls=NumpyEncoder)\n",
    "\n",
    "    def test(self):\n",
    "        testing_mem = {\"Actions\": [], \"Purchases\": [], \"Sales\": [], \"Inventory Size\": [], \"Portfolio Value\": [], \"Balance\": [], \"Reward\": []}\n",
    "        t0 = time()\n",
    "        self.NUM_EPISODES = len(self.data_manager.test_data_dict)\n",
    "        \n",
    "        for accession_number, data in self.data_manager.test_data_dict.items():\n",
    "            normalized_df, reference_df = data\n",
    "            self.test_df_shape = normalized_df.shape\n",
    "            self.trading_agent = Agent(self.test_df_shape, self.NUM_EPISODES)\n",
    "            self.trading_agent.epsilon = 0\n",
    "            self.trading_agent.inventory = []\n",
    "            state = self.trading_agent.get_state(0, normalized_df)\n",
    "            balance = 0\n",
    "            portfolio_value = 0\n",
    "            self.trading_agent.portfolio = [0, balance, portfolio_value]\n",
    "\n",
    "            for t in range(len(normalized_df) - 1):\n",
    "                action = self.trading_agent.get_action(state, t, reference_df)\n",
    "\n",
    "                if action == 0:\n",
    "                    testing_mem[testing_mem][\"Sales\"].append(reference_df['close'].iloc[t])\n",
    "                elif action == 2:\n",
    "                    testing_mem[testing_mem][\"Purchases\"].append(reference_df['close'].iloc[t])\n",
    "\n",
    "                next_state = self.trading_agent.get_state(t + 1, normalized_df)\n",
    "                reward = self.trading_agent.trade(t, action, normalized_df, reference_df, len(normalized_df)-1, trading_fee_rate=0.01)\n",
    "                \n",
    "                state = next_state\n",
    "                assets_held, balance, portfolio_value = self.trading_agent.portfolio\n",
    "\n",
    "                testing_mem[\"Actions\"].append(int(action))\n",
    "                testing_mem[\"Inventory Size\"].append(len(self.trading_agent.inventory))\n",
    "                testing_mem[\"Portfolio Value\"].append(float(portfolio_value))\n",
    "                testing_mem[\"Balance\"].append(float(balance))\n",
    "                testing_mem[\"Reward\"].append(float(reward))\n",
    "                if t % 10 == 0:\n",
    "                    print(f\"\"\"\n",
    "                            Time step {t} / {len(normalized_df)}   \n",
    "                            |  Inventory Size: {len(self.trading_agent.inventory)}  \n",
    "                            |  {reference_df['ISSUERTRADINGSYMBOL'].iloc[t]}: ${round(reference_df['close'].iloc[t], 3)}\n",
    "                            |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}   \n",
    "                            |  Balance: {round(testing_mem['Balance'][t], 3)}\n",
    "                            |  Action: {int(action)}  |  Reward: {round(reward, 3)}\n",
    "                            |  Reward: {round(reward, 3)}\n",
    "                            \"\"\")\n",
    "    \n",
    "        if self.local_rank == 0:\n",
    "\n",
    "            with open('Output/testing_scores.out', 'a') as f:\n",
    "                f.write(f\"\"\"TESTING (runtime: {time() - t0})   |  \n",
    "                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\n",
    "                        \"\"\")\n",
    "            with open(\"Output/testing_mem.json\", 'w') as f:\n",
    "                json.dump(testing_mem, f, cls=NumpyEncoder)\n",
    "        \n",
    "        elif torch.distributed.is_available() and torch.distributed.is_initialized() and dist.rank == 0:\n",
    "            with open('Output/testing_scores.out', 'a') as f:\n",
    "                f.write(f\"\"\"TESTING (runtime: {time() - t0})   |  \n",
    "                        Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\n",
    "                        \"\"\")\n",
    "            with open(\"Output/testing_mem.json\", 'w') as f:\n",
    "                json.dump(testing_mem, f, cls=NumpyEncoder)\n",
    "\n",
    "#     def run(self, directory):\n",
    "#         print(\"PyTorch version \" + torch.__version__)\n",
    "#         print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "#         # grab the gpu id if available\n",
    "#         print(\"GPU available: \", torch.cuda.is_available())\n",
    "#         print(\"GPU device: \", torch.cuda.get_device_name(0))\n",
    "#         print(\"GPU device count: \", torch.cuda.device_count())\n",
    "#         print(\"GPU device index: \", torch.cuda.current_device())\n",
    "\n",
    "#         print(torch.cuda.is_available())\n",
    "\n",
    "#         self.local_rank = setup_distributed(self.local_rank)\n",
    "\n",
    "#         self.load_and_prepare_data(directory)\n",
    "#         self.train()\n",
    "#         self.test()\n",
    "\n",
    "#         # Only save models or log information from one process to avoid conflicts\n",
    "#         if dist.get_rank() == 0:\n",
    "#             self.trading_agent.save_model()\n",
    "#             print(\"Model saved\")\n",
    "\n",
    "#         self.data_visualizer = DataVisualizer()\n",
    "#         self.data_visualizer.visualize_data()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the distributed environment\n",
    "#     local_rank = 0\n",
    "\n",
    "#     # Create a TradingSystem instance\n",
    "#     trading_system = TradingSystem(local_rank=local_rank)\n",
    "\n",
    "#     directory = \"C:\\\\Users\\\\ericb\\\\Desktop\\\\CS 535 - Big Data\\\\Term_Projcet\\\\Storm_ouput\\\\Data_Input\\\\Test\\\\\"\n",
    "#     # Run the trading system\n",
    "#     trading_system.run(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.13.1\n",
      "Num GPUs Available:  1\n",
      "GPU available:  True\n",
      "GPU device:  NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "GPU device count:  1\n",
      "GPU device index:  0\n",
      "True\n",
      "new_storm_output_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\anaconda3\\envs\\ML_Torch\\lib\\site-packages\\ipykernel_launcher.py:63: DtypeWarning: Columns (0,1,2,3,4,9,76) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes total : 84\n",
      "train_df_shape (494, 96)\n",
      "Loading the model\n",
      "\n",
      "===== Episode 1 : 0001104659-15-040483 =====\n",
      "494\n",
      "\n",
      "                            Time step 0 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $53.79   \n",
      "                            |   Epsilon: 1.0   \n",
      "                            |   MSE Loss: 0\n",
      "                            |   Last 3 Rewards: [0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 50 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $54.62   \n",
      "                            |   Epsilon: 1.0   \n",
      "                            |   MSE Loss: 0\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 100 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $54.44   \n",
      "                            |   Epsilon: 0.9962   \n",
      "                            |   MSE Loss: 0.09169616550207138\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 150 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $52.3   \n",
      "                            |   Epsilon: 0.9912   \n",
      "                            |   MSE Loss: 0.04282566159963608\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 200 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $46.93   \n",
      "                            |   Epsilon: 0.9863   \n",
      "                            |   MSE Loss: 0.021368760615587234\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 250 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $44.14   \n",
      "                            |   Epsilon: 0.9814   \n",
      "                            |   MSE Loss: 0.02580045536160469\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 300 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $45.14   \n",
      "                            |   Epsilon: 0.9765   \n",
      "                            |   MSE Loss: 0.019912028685212135\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 350 / 494   \n",
      "                            |   Inventory Size: 1\n",
      "                            |   Last 3 Actions: [2, 0, 2]\n",
      "                            |   Portfolio Value: 0.0 \n",
      "                            |   Balance: -2.27\n",
      "                            |   ENV: $44.82   \n",
      "                            |   Epsilon: 0.9716   \n",
      "                            |   MSE Loss: 1.1764767169952393\n",
      "                            |   Last 3 Rewards: [-17.727533586040334, 2.664026684764195, -0.12222229982280203]\n",
      "                            |   Average Reward: 0.17282556420720296\n",
      "                            |   Max Reward: 21.193269466879624\n",
      "                            |   Min Reward: -17.727533586040334\n",
      "                            \n",
      "\n",
      "                            Time step 400 / 494   \n",
      "                            |   Inventory Size: 2\n",
      "                            |   Last 3 Actions: [0, 2, 2]\n",
      "                            |   Portfolio Value: 0.03 \n",
      "                            |   Balance: -4.669\n",
      "                            |   ENV: $43.44   \n",
      "                            |   Epsilon: 0.9668   \n",
      "                            |   MSE Loss: 3.3493494987487793\n",
      "                            |   Last 3 Rewards: [2.2803632935833758, -0.07952217902493589, 19.519293035680693]\n",
      "                            |   Average Reward: 0.36236598773809603\n",
      "                            |   Max Reward: 21.193269466879624\n",
      "                            |   Min Reward: -21.25389408867219\n",
      "                            \n",
      "\n",
      "                            Time step 450 / 494   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [2, 2, 0]\n",
      "                            |   Portfolio Value: 0.0 \n",
      "                            |   Balance: -2.309\n",
      "                            |   ENV: $43.76   \n",
      "                            |   Epsilon: 0.9619   \n",
      "                            |   MSE Loss: 6.094204902648926\n",
      "                            |   Last 3 Rewards: [-0.03913976901298338, -19.13852054217086, 1.6661913400636437]\n",
      "                            |   Average Reward: 0.39870120365148265\n",
      "                            |   Max Reward: 21.193269466879624\n",
      "                            |   Min Reward: -21.25389408867219\n",
      "                            \n",
      "train_df_shape (442, 96)\n",
      "Loading the model\n",
      "\n",
      "===== Episode 2 : 0001104659-15-060148 =====\n",
      "442\n",
      "\n",
      "                            Time step 0 / 442   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $41.65   \n",
      "                            |   Epsilon: 1.0   \n",
      "                            |   MSE Loss: 0\n",
      "                            |   Last 3 Rewards: [0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 50 / 442   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $44.0   \n",
      "                            |   Epsilon: 1.0   \n",
      "                            |   MSE Loss: 0\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 100 / 442   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $43.65   \n",
      "                            |   Epsilon: 0.9962   \n",
      "                            |   MSE Loss: 0.08673050999641418\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n",
      "\n",
      "                            Time step 150 / 442   \n",
      "                            |   Inventory Size: 0\n",
      "                            |   Last 3 Actions: [3, 3, 3]\n",
      "                            |   Portfolio Value: 0 \n",
      "                            |   Balance: 0\n",
      "                            |   ENV: $44.99   \n",
      "                            |   Epsilon: 0.9912   \n",
      "                            |   MSE Loss: 0.03288061171770096\n",
      "                            |   Last 3 Rewards: [0, 0, 0]\n",
      "                            |   Average Reward: 0.0\n",
      "                            |   Max Reward: 0\n",
      "                            |   Min Reward: 0\n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version \" + torch.__version__)\n",
    "print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "# grab the gpu id if available\n",
    "print(\"GPU available: \", torch.cuda.is_available())\n",
    "print(\"GPU device: \", torch.cuda.get_device_name(0))\n",
    "print(\"GPU device count: \", torch.cuda.device_count())\n",
    "print(\"GPU device index: \", torch.cuda.current_device())\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Initialize the distributed environment\n",
    "local_rank = 0\n",
    "\n",
    "# Create a TradingSystem instance\n",
    "trading_system = TradingSystem(local_rank=local_rank)\n",
    "\n",
    "directory = \"C:\\\\Users\\\\ericb\\\\Desktop\\\\CS 535 - Big Data\\\\Term_Projcet\\\\Storm_ouput\\\\Data_Input\\\\Test\\\\\"\n",
    "# Run the trading system\n",
    "\n",
    "trading_system.local_rank = setup_distributed(trading_system.local_rank)\n",
    "\n",
    "trading_system.load_and_prepare_data(directory)\n",
    "trading_system.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_system.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9448\\367440343.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrading_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_visualizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataVisualizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrading_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_visualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ericb\\Desktop\\CS 535 - Big Data\\Term_Projcet\\DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot\\DDDQ_RL_Trading_Bot\\Code\\DataVisualizer.py\u001b[0m in \u001b[0;36mvisualize_data\u001b[1;34m(self, episode_interval)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# Plot the profit data for all episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_realized_profit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_episode_profits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Realized Profit for All Training Episodes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Output/all_episodes_profit.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Continue with the rest of the original visualize_data method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ericb\\Desktop\\CS 535 - Big Data\\Term_Projcet\\DDDQN_RL_AI_Insider_Trading_Exploitation_Trading_Bot\\DDDQ_RL_Trading_Bot\\Code\\DataVisualizer.py\u001b[0m in \u001b[0;36mplot_realized_profit\u001b[1;34m(self, data, title, filename)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot_realized_profit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Realized Profit\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Realized Profit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Profit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trading_system.data_visualizer = DataVisualizer()\n",
    "trading_system.data_visualizer.visualize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_AI_RL_20230410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
